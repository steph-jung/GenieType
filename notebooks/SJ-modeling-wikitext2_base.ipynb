{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-text-data\" data-toc-modified-id=\"Load-text-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load text data</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:29:57.671166Z",
     "start_time": "2020-06-24T19:29:57.651266Z"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T22:38:00.437247Z",
     "start_time": "2020-06-24T22:38:00.431496Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:29:58.675233Z",
     "start_time": "2020-06-24T19:29:58.672139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:29:01.224076Z",
     "start_time": "2020-06-25T00:29:01.176779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5e680228b0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:30:03.028261Z",
     "start_time": "2020-06-24T19:30:03.023151Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Add word to 'self.idx2word' and 'self.word2idx'.\n",
    "        \"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1 # starts from 0\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:30:05.627540Z",
     "start_time": "2020-06-24T19:30:05.620440Z"
    }
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "        \n",
    "    def tokenize(self, path):\n",
    "        \"\"\"\n",
    "        Tokenize a text file and add tokens to the dictionary.\n",
    "        \"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        \n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idx_all = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                idx_line = []\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                    idx_line.append(self.dictionary.word2idx[word])\n",
    "                idx_all.append(torch.tensor(idx_line).type(torch.int64))\n",
    "            ids = torch.cat(idx_all)               \n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:30:08.956575Z",
     "start_time": "2020-06-24T19:30:07.355751Z"
    }
   },
   "outputs": [],
   "source": [
    "model_data_filepath = 'data/'\n",
    "\n",
    "corpus = Corpus(model_data_filepath + 'wikitext-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:30:08.961614Z",
     "start_time": "2020-06-24T19:30:08.958555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.dictionary.word2idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T21:32:49.466389Z",
     "start_time": "2020-06-24T21:32:49.461912Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_batch(data, n_seq):\n",
    "    \"\"\"\n",
    "    Trim data and cleanly divide data into n_seq chunks.\n",
    "    \"\"\"\n",
    "    nbatch = data.size(0) // n_seq\n",
    "    \n",
    "    # Trim off remainders.\n",
    "    data = data.narrow(0, 0, nbatch * n_seq)\n",
    "    \n",
    "    # Evenly divide the data across the n_seq batches.\n",
    "    # Shape : ([bptt, n_seq])\n",
    "    return data.view(n_seq, -1).t().contiguous().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batch(source, i):\n",
    "#     \"\"\"\n",
    "#     Subdivides the source into chunks of length bptt.\n",
    "#     The chunks are along dimension 0 (length of each row is n_seq).\n",
    "#     \"\"\"\n",
    "#     seq_len = min(bptt, len(source)-1-i)\n",
    "#     data = source[i:i+seq_len]\n",
    "#     target = source[i+1:i+1+seq_len].view(-1)\n",
    "#     return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T22:47:26.866671Z",
     "start_time": "2020-06-24T22:47:26.862304Z"
    }
   },
   "outputs": [],
   "source": [
    "# class wikiDataset(Dataset):\n",
    "#     def __init__(self, data, n_seq):\n",
    "#         self.data = data\n",
    "#         self.n_seq = n_seq\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         data = self.data[idx]\n",
    "#         return make_batch(data, self.n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T22:47:38.734455Z",
     "start_time": "2020-06-24T22:47:38.731701Z"
    }
   },
   "outputs": [],
   "source": [
    "# wiki_train = wikiDataset(corpus.train, n_seq=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T22:49:17.763387Z",
     "start_time": "2020-06-24T22:49:17.719307Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(wiki_train, batch_size=35, shuffle=False)\n",
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:29:44.439483Z",
     "start_time": "2020-06-25T00:29:44.436710Z"
    }
   },
   "outputs": [],
   "source": [
    "n_seq = 20\n",
    "eval_n_seq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:29:54.600907Z",
     "start_time": "2020-06-25T00:29:54.536224Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = make_batch(corpus.train, n_seq)\n",
    "val_data = make_batch(corpus.valid, eval_n_seq)\n",
    "test_data = make_batch(corpus.test, eval_n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:29:54.986636Z",
     "start_time": "2020-06-25T00:29:54.977828Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   284, 15178,  ...,  1352,  1335,    16],\n",
       "        [    1,   357,    43,  ...,    46,    43,  2015],\n",
       "        [    2,  1496,  7369,  ...,   380,    27, 33001],\n",
       "        ...,\n",
       "        [  357,   415,   173,  ...,   212,    78,  1575],\n",
       "        [ 2520,     9,  3890,  ...,   208,    27,   808],\n",
       "        [   33,    35,    19,  ...,  8832,  6091,   209]], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T21:33:48.889960Z",
     "start_time": "2020-06-24T21:33:48.878459Z"
    }
   },
   "outputs": [],
   "source": [
    "class NWPMModel(nn.Module):\n",
    "    def __init__(self, ntoken, nemb, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, nemb)\n",
    "        self.rnn = nn.GRU(nemb, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        \n",
    "        if tie_weights:\n",
    "            if nhid != nemb:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to nemb')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "    \n",
    "    def init_hidden(self, n_seq):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.nlayers, n_seq, self.nhid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:30:41.708192Z",
     "start_time": "2020-06-25T00:30:41.697211Z"
    }
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"\n",
    "    Wraps hidden states in new Tensors, to detach them from their history.\n",
    "    \"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:30:41.995110Z",
     "start_time": "2020-06-25T00:30:41.987986Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    \"\"\"\n",
    "    Subdivides the source into chunks of length bptt.\n",
    "    The chunks are along dimension 0 (length of each row is n_seq).\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source)-1-i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:32:33.507553Z",
     "start_time": "2020-06-25T00:32:33.497947Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(dataset, ntokens=len(corpus.dictionary), bptt=bptt):\n",
    "    model.eval() # disables dropout\n",
    "    sum_loss = 0\n",
    "    hidden = model.init_hidden(eval_n_seq)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, dataset.size(0)-1, bptt):\n",
    "            X, y = get_batch(dataset, i)\n",
    "            y_hat, hidden = model(X, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            sum_loss += len(X) * criterion(y_hat, y).item()\n",
    "    return sum_loss / (len(dataset)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:36:51.551917Z",
     "start_time": "2020-06-25T00:36:51.538461Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epochs(optimizer, scheduler, n_seq, ntokens=len(corpus.dictionary), epochs=20, log_interval=200):\n",
    "    \"\"\"\n",
    "    Train for epochs.\n",
    "    \"\"\"\n",
    "    best_val_loss = None\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        sum_loss = 0.\n",
    "        hidden = model.init_hidden(n_seq)\n",
    "\n",
    "        for batch, i in enumerate(range(0, train_data.size(0)-1, bptt)):\n",
    "            X, y = get_batch(train_data, i)\n",
    "            \n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            optimizer.zero_grad()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            y_hat, hidden = model(X, hidden)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 'clip_grad_norm' helps prevent the exploding gradient problem in RNNs/LSTMs.\n",
    "            lr = scheduler.get_lr()[0]\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            if batch % log_interval == 0 and batch > 0:\n",
    "                cur_loss = sum_loss / log_interval\n",
    "                print(f'| {epoch=:3d} | {batch:5d}/{len(train_data)//bptt} batches |  {lr=:2.2f}  |  '\n",
    "                f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
    "                sum_loss = 0\n",
    "                start_time = time.time()\n",
    "        \n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-'*89)\n",
    "        print(f'| end of {epoch=:2d} | time: {time.time()-epoch_start_time:5.2f}s | {val_loss=:5.2f} | '\n",
    "              f'valid ppl {math.exp(val_loss):8.2f}')\n",
    "        print('-' * 89)\n",
    "        \n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "    os.rename('model.pth', f'model_val_loss_{val_loss:.2f}_val_ppl_{math.exp(val_loss):.2f}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:30:21.205896Z",
     "start_time": "2020-06-25T00:30:21.201318Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "ntokens = len(corpus.dictionary)\n",
    "bptt = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:30:21.852745Z",
     "start_time": "2020-06-25T00:30:21.654082Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NWPMModel(\n",
    "    ntoken = ntokens,\n",
    "    nemb = 650,\n",
    "    nhid = 650,\n",
    "    nlayers=2,\n",
    "    dropout=0.2,\n",
    "    tie_weights=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:37:23.189077Z",
     "start_time": "2020-06-25T00:37:23.182449Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=5, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5,\n",
    "                                                steps_per_epoch=len(train_data)//bptt, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T00:51:14.550195Z",
     "start_time": "2020-06-25T00:37:24.293712Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch=  1 |   200/2983 batches |  lr=0.20  |  loss  7.55 | ppl  1907.99\n",
      "| epoch=  1 |   400/2983 batches |  lr=0.21  |  loss  6.71 | ppl   819.47\n",
      "| epoch=  1 |   600/2983 batches |  lr=0.21  |  loss  6.46 | ppl   637.97\n",
      "| epoch=  1 |   800/2983 batches |  lr=0.22  |  loss  6.34 | ppl   568.69\n",
      "| epoch=  1 |  1000/2983 batches |  lr=0.24  |  loss  6.22 | ppl   504.36\n",
      "| epoch=  1 |  1200/2983 batches |  lr=0.25  |  loss  6.16 | ppl   474.87\n",
      "| epoch=  1 |  1400/2983 batches |  lr=0.27  |  loss  6.08 | ppl   438.23\n",
      "| epoch=  1 |  1600/2983 batches |  lr=0.29  |  loss  6.08 | ppl   435.68\n",
      "| epoch=  1 |  1800/2983 batches |  lr=0.32  |  loss  5.93 | ppl   375.07\n",
      "| epoch=  1 |  2000/2983 batches |  lr=0.35  |  loss  5.91 | ppl   369.91\n",
      "| epoch=  1 |  2200/2983 batches |  lr=0.38  |  loss  5.80 | ppl   328.99\n",
      "| epoch=  1 |  2400/2983 batches |  lr=0.41  |  loss  5.80 | ppl   329.10\n",
      "| epoch=  1 |  2600/2983 batches |  lr=0.45  |  loss  5.77 | ppl   320.38\n",
      "| epoch=  1 |  2800/2983 batches |  lr=0.48  |  loss  5.66 | ppl   288.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch= 1 | time: 337.65s | val_loss= 5.61 | valid ppl   272.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch=  2 |   200/2983 batches |  lr=0.57  |  loss  5.65 | ppl   283.30\n",
      "| epoch=  2 |   400/2983 batches |  lr=0.61  |  loss  5.61 | ppl   272.75\n",
      "| epoch=  2 |   600/2983 batches |  lr=0.66  |  loss  5.44 | ppl   230.28\n",
      "| epoch=  2 |   800/2983 batches |  lr=0.71  |  loss  5.47 | ppl   236.58\n",
      "| epoch=  2 |  1000/2983 batches |  lr=0.76  |  loss  5.42 | ppl   226.87\n",
      "| epoch=  2 |  1200/2983 batches |  lr=0.82  |  loss  5.41 | ppl   223.60\n",
      "| epoch=  2 |  1400/2983 batches |  lr=0.88  |  loss  5.42 | ppl   225.47\n",
      "| epoch=  2 |  1600/2983 batches |  lr=0.94  |  loss  5.48 | ppl   239.31\n",
      "| epoch=  2 |  1800/2983 batches |  lr=1.00  |  loss  5.33 | ppl   206.62\n",
      "| epoch=  2 |  2000/2983 batches |  lr=1.06  |  loss  5.35 | ppl   210.94\n",
      "| epoch=  2 |  2200/2983 batches |  lr=1.13  |  loss  5.24 | ppl   189.39\n",
      "| epoch=  2 |  2400/2983 batches |  lr=1.19  |  loss  5.27 | ppl   194.66\n",
      "| epoch=  2 |  2600/2983 batches |  lr=1.26  |  loss  5.27 | ppl   194.01\n",
      "| epoch=  2 |  2800/2983 batches |  lr=1.33  |  loss  5.19 | ppl   179.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch= 2 | time: 336.17s | val_loss= 5.33 | valid ppl   206.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch=  3 |   200/2983 batches |  lr=1.47  |  loss  5.22 | ppl   185.05\n",
      "| epoch=  3 |   400/2983 batches |  lr=1.55  |  loss  5.23 | ppl   186.45\n",
      "| epoch=  3 |   600/2983 batches |  lr=1.63  |  loss  5.04 | ppl   155.17\n",
      "| epoch=  3 |   800/2983 batches |  lr=1.70  |  loss  5.10 | ppl   164.62\n",
      "| epoch=  3 |  1000/2983 batches |  lr=1.78  |  loss  5.09 | ppl   162.51\n",
      "| epoch=  3 |  1200/2983 batches |  lr=1.86  |  loss  5.09 | ppl   163.03\n",
      "| epoch=  3 |  1400/2983 batches |  lr=1.94  |  loss  5.12 | ppl   167.66\n",
      "| epoch=  3 |  1600/2983 batches |  lr=2.02  |  loss  5.19 | ppl   180.17\n",
      "| epoch=  3 |  1800/2983 batches |  lr=2.11  |  loss  5.07 | ppl   159.15\n",
      "| epoch=  3 |  2000/2983 batches |  lr=2.19  |  loss  5.10 | ppl   163.84\n",
      "| epoch=  3 |  2200/2983 batches |  lr=2.27  |  loss  4.98 | ppl   145.91\n",
      "| epoch=  3 |  2400/2983 batches |  lr=2.36  |  loss  5.03 | ppl   152.31\n",
      "| epoch=  3 |  2600/2983 batches |  lr=2.44  |  loss  5.05 | ppl   156.52\n",
      "| epoch=  3 |  2800/2983 batches |  lr=2.52  |  loss  4.98 | ppl   146.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch= 3 | time: 336.99s | val_loss= 5.33 | valid ppl   206.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch=  4 |   200/2983 batches |  lr=2.69  |  loss  5.05 | ppl   155.63\n",
      "| epoch=  4 |   400/2983 batches |  lr=2.77  |  loss  5.06 | ppl   157.62\n",
      "| epoch=  4 |   600/2983 batches |  lr=2.85  |  loss  4.90 | ppl   133.63\n",
      "| epoch=  4 |   800/2983 batches |  lr=2.94  |  loss  4.96 | ppl   142.95\n",
      "| epoch=  4 |  1000/2983 batches |  lr=3.02  |  loss  4.98 | ppl   144.93\n",
      "| epoch=  4 |  1200/2983 batches |  lr=3.10  |  loss  4.99 | ppl   146.87\n",
      "| epoch=  4 |  1400/2983 batches |  lr=3.19  |  loss  5.04 | ppl   154.41\n",
      "| epoch=  4 |  1600/2983 batches |  lr=3.27  |  loss  5.11 | ppl   166.41\n",
      "| epoch=  4 |  1800/2983 batches |  lr=3.35  |  loss  5.01 | ppl   150.58\n",
      "| epoch=  4 |  2000/2983 batches |  lr=3.43  |  loss  5.06 | ppl   157.45\n",
      "| epoch=  4 |  2200/2983 batches |  lr=3.51  |  loss  4.96 | ppl   142.38\n",
      "| epoch=  4 |  2400/2983 batches |  lr=3.58  |  loss  5.02 | ppl   151.24\n",
      "| epoch=  4 |  2600/2983 batches |  lr=3.66  |  loss  5.06 | ppl   157.36\n",
      "| epoch=  4 |  2800/2983 batches |  lr=3.73  |  loss  5.04 | ppl   153.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch= 4 | time: 337.54s | val_loss= 5.38 | valid ppl   217.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch=  5 |   200/2983 batches |  lr=3.87  |  loss  5.20 | ppl   182.08\n",
      "| epoch=  5 |   400/2983 batches |  lr=3.94  |  loss  5.33 | ppl   205.70\n",
      "| epoch=  5 |   600/2983 batches |  lr=4.01  |  loss  6.67 | ppl   792.25\n",
      "| epoch=  5 |   800/2983 batches |  lr=4.08  |  loss  6.88 | ppl   974.86\n",
      "| epoch=  5 |  1000/2983 batches |  lr=4.15  |  loss  6.75 | ppl   856.95\n",
      "| epoch=  5 |  1200/2983 batches |  lr=4.21  |  loss  6.80 | ppl   895.01\n",
      "| epoch=  5 |  1400/2983 batches |  lr=4.27  |  loss  6.63 | ppl   760.45\n",
      "| epoch=  5 |  1600/2983 batches |  lr=4.33  |  loss  6.73 | ppl   838.82\n",
      "| epoch=  5 |  1800/2983 batches |  lr=4.39  |  loss  6.66 | ppl   778.93\n",
      "| epoch=  5 |  2000/2983 batches |  lr=4.44  |  loss  6.70 | ppl   811.36\n",
      "| epoch=  5 |  2200/2983 batches |  lr=4.50  |  loss  6.57 | ppl   712.72\n",
      "| epoch=  5 |  2400/2983 batches |  lr=4.55  |  loss  6.66 | ppl   779.05\n",
      "| epoch=  5 |  2600/2983 batches |  lr=4.59  |  loss  6.62 | ppl   751.62\n",
      "| epoch=  5 |  2800/2983 batches |  lr=4.64  |  loss  6.54 | ppl   689.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch= 5 | time: 337.32s | val_loss= 6.67 | valid ppl   788.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch=  6 |   200/2983 batches |  lr=4.72  |  loss  6.47 | ppl   644.17\n",
      "| epoch=  6 |   400/2983 batches |  lr=4.76  |  loss  6.64 | ppl   768.29\n",
      "| epoch=  6 |   600/2983 batches |  lr=4.79  |  loss  6.57 | ppl   715.34\n",
      "| epoch=  6 |   800/2983 batches |  lr=4.83  |  loss  6.59 | ppl   731.03\n",
      "| epoch=  6 |  1000/2983 batches |  lr=4.86  |  loss  6.59 | ppl   724.96\n",
      "| epoch=  6 |  1200/2983 batches |  lr=4.88  |  loss  6.67 | ppl   791.68\n",
      "| epoch=  6 |  1400/2983 batches |  lr=4.91  |  loss  6.67 | ppl   791.28\n",
      "| epoch=  6 |  1600/2983 batches |  lr=4.93  |  loss  6.83 | ppl   924.74\n",
      "| epoch=  6 |  1800/2983 batches |  lr=4.95  |  loss  6.70 | ppl   809.99\n",
      "| epoch=  6 |  2000/2983 batches |  lr=4.96  |  loss  6.73 | ppl   839.31\n",
      "| epoch=  6 |  2200/2983 batches |  lr=4.98  |  loss  6.67 | ppl   789.83\n",
      "| epoch=  6 |  2400/2983 batches |  lr=4.99  |  loss  6.75 | ppl   858.26\n",
      "| epoch=  6 |  2600/2983 batches |  lr=4.99  |  loss  6.79 | ppl   892.12\n",
      "| epoch=  6 |  2800/2983 batches |  lr=5.00  |  loss  6.74 | ppl   845.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch= 6 | time: 334.81s | val_loss= 6.48 | valid ppl   651.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch=  7 |   200/2983 batches |  lr=5.00  |  loss  6.39 | ppl   594.29\n",
      "| epoch=  7 |   400/2983 batches |  lr=5.00  |  loss  6.52 | ppl   678.94\n",
      "| epoch=  7 |   600/2983 batches |  lr=5.00  |  loss  6.48 | ppl   649.30\n",
      "| epoch=  7 |   800/2983 batches |  lr=5.00  |  loss  6.53 | ppl   683.40\n",
      "| epoch=  7 |  1000/2983 batches |  lr=4.99  |  loss  6.54 | ppl   691.70\n",
      "| epoch=  7 |  1200/2983 batches |  lr=4.99  |  loss  6.64 | ppl   768.34\n",
      "| epoch=  7 |  1400/2983 batches |  lr=4.99  |  loss  6.69 | ppl   803.04\n",
      "| epoch=  7 |  1600/2983 batches |  lr=4.98  |  loss  6.79 | ppl   887.94\n",
      "| epoch=  7 |  1800/2983 batches |  lr=4.98  |  loss  6.70 | ppl   813.84\n",
      "| epoch=  7 |  2000/2983 batches |  lr=4.97  |  loss  6.64 | ppl   765.92\n",
      "| epoch=  7 |  2200/2983 batches |  lr=4.97  |  loss  6.57 | ppl   711.37\n",
      "| epoch=  7 |  2400/2983 batches |  lr=4.96  |  loss  6.64 | ppl   765.09\n",
      "| epoch=  7 |  2600/2983 batches |  lr=4.95  |  loss  6.64 | ppl   766.55\n",
      "| epoch=  7 |  2800/2983 batches |  lr=4.94  |  loss  6.61 | ppl   739.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch= 7 | time: 335.78s | val_loss= 6.35 | valid ppl   574.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch=  8 |   200/2983 batches |  lr=4.93  |  loss  6.21 | ppl   496.19\n",
      "| epoch=  8 |   400/2983 batches |  lr=4.92  |  loss  6.35 | ppl   575.12\n",
      "| epoch=  8 |   600/2983 batches |  lr=4.91  |  loss  6.29 | ppl   539.19\n",
      "| epoch=  8 |   800/2983 batches |  lr=4.90  |  loss  6.33 | ppl   560.46\n",
      "| epoch=  8 |  1000/2983 batches |  lr=4.89  |  loss  6.40 | ppl   600.17\n",
      "| epoch=  8 |  1200/2983 batches |  lr=4.88  |  loss  6.47 | ppl   645.82\n",
      "| epoch=  8 |  1400/2983 batches |  lr=4.86  |  loss  6.52 | ppl   678.97\n",
      "| epoch=  8 |  1600/2983 batches |  lr=4.85  |  loss  6.53 | ppl   682.35\n",
      "| epoch=  8 |  1800/2983 batches |  lr=4.84  |  loss  6.40 | ppl   599.12\n",
      "| epoch=  8 |  2000/2983 batches |  lr=4.83  |  loss  6.46 | ppl   637.62\n",
      "| epoch=  8 |  2200/2983 batches |  lr=4.81  |  loss  6.34 | ppl   567.08\n",
      "| epoch=  8 |  2400/2983 batches |  lr=4.80  |  loss  6.37 | ppl   582.23\n",
      "| epoch=  8 |  2600/2983 batches |  lr=4.78  |  loss  6.39 | ppl   598.68\n",
      "| epoch=  8 |  2800/2983 batches |  lr=4.77  |  loss  6.38 | ppl   591.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch= 8 | time: 338.49s | val_loss= 6.60 | valid ppl   737.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch=  9 |   200/2983 batches |  lr=4.74  |  loss  6.17 | ppl   477.21\n",
      "| epoch=  9 |   400/2983 batches |  lr=4.72  |  loss  6.24 | ppl   512.76\n",
      "| epoch=  9 |   600/2983 batches |  lr=4.70  |  loss  6.14 | ppl   461.89\n",
      "| epoch=  9 |   800/2983 batches |  lr=4.68  |  loss  6.17 | ppl   478.77\n",
      "| epoch=  9 |  1000/2983 batches |  lr=4.66  |  loss  6.23 | ppl   508.82\n",
      "| epoch=  9 |  1200/2983 batches |  lr=4.64  |  loss  6.28 | ppl   532.49\n",
      "| epoch=  9 |  1400/2983 batches |  lr=4.63  |  loss  6.29 | ppl   537.98\n",
      "| epoch=  9 |  1600/2983 batches |  lr=4.60  |  loss  6.38 | ppl   587.00\n",
      "| epoch=  9 |  1800/2983 batches |  lr=4.58  |  loss  6.24 | ppl   512.90\n",
      "| epoch=  9 |  2000/2983 batches |  lr=4.56  |  loss  6.27 | ppl   526.82\n",
      "| epoch=  9 |  2200/2983 batches |  lr=4.54  |  loss  6.18 | ppl   482.08\n",
      "| epoch=  9 |  2400/2983 batches |  lr=4.52  |  loss  6.26 | ppl   525.56\n",
      "| epoch=  9 |  2600/2983 batches |  lr=4.50  |  loss  6.30 | ppl   543.20\n",
      "| epoch=  9 |  2800/2983 batches |  lr=4.47  |  loss  6.25 | ppl   519.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch= 9 | time: 337.19s | val_loss= 6.33 | valid ppl   563.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 10 |   200/2983 batches |  lr=4.43  |  loss  6.13 | ppl   461.18\n",
      "| epoch= 10 |   400/2983 batches |  lr=4.41  |  loss  6.28 | ppl   536.12\n",
      "| epoch= 10 |   600/2983 batches |  lr=4.38  |  loss  6.16 | ppl   472.37\n",
      "| epoch= 10 |   800/2983 batches |  lr=4.36  |  loss  6.23 | ppl   509.39\n",
      "| epoch= 10 |  1000/2983 batches |  lr=4.33  |  loss  6.28 | ppl   536.32\n",
      "| epoch= 10 |  1200/2983 batches |  lr=4.30  |  loss  6.36 | ppl   579.46\n",
      "| epoch= 10 |  1400/2983 batches |  lr=4.28  |  loss  6.40 | ppl   602.76\n",
      "| epoch= 10 |  1600/2983 batches |  lr=4.25  |  loss  6.46 | ppl   638.52\n",
      "| epoch= 10 |  1800/2983 batches |  lr=4.22  |  loss  6.37 | ppl   585.18\n",
      "| epoch= 10 |  2000/2983 batches |  lr=4.20  |  loss  6.39 | ppl   596.79\n",
      "| epoch= 10 |  2200/2983 batches |  lr=4.17  |  loss  6.28 | ppl   531.51\n",
      "| epoch= 10 |  2400/2983 batches |  lr=4.14  |  loss  6.33 | ppl   563.04\n",
      "| epoch= 10 |  2600/2983 batches |  lr=4.11  |  loss  6.35 | ppl   575.15\n",
      "| epoch= 10 |  2800/2983 batches |  lr=4.08  |  loss  6.30 | ppl   544.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=10 | time: 338.32s | val_loss= 6.38 | valid ppl   587.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 11 |   200/2983 batches |  lr=4.03  |  loss  6.03 | ppl   413.73\n",
      "| epoch= 11 |   400/2983 batches |  lr=4.00  |  loss  6.06 | ppl   428.38\n",
      "| epoch= 11 |   600/2983 batches |  lr=3.97  |  loss  5.95 | ppl   384.36\n",
      "| epoch= 11 |   800/2983 batches |  lr=3.94  |  loss  5.99 | ppl   400.72\n",
      "| epoch= 11 |  1000/2983 batches |  lr=3.91  |  loss  6.04 | ppl   420.66\n",
      "| epoch= 11 |  1200/2983 batches |  lr=3.87  |  loss  6.11 | ppl   450.00\n",
      "| epoch= 11 |  1400/2983 batches |  lr=3.84  |  loss  6.11 | ppl   451.54\n",
      "| epoch= 11 |  1600/2983 batches |  lr=3.81  |  loss  6.21 | ppl   495.66\n",
      "| epoch= 11 |  1800/2983 batches |  lr=3.78  |  loss  6.12 | ppl   455.97\n",
      "| epoch= 11 |  2000/2983 batches |  lr=3.75  |  loss  6.18 | ppl   482.38\n",
      "| epoch= 11 |  2200/2983 batches |  lr=3.71  |  loss  6.09 | ppl   442.96\n",
      "| epoch= 11 |  2400/2983 batches |  lr=3.68  |  loss  6.15 | ppl   468.41\n",
      "| epoch= 11 |  2600/2983 batches |  lr=3.65  |  loss  6.15 | ppl   469.81\n",
      "| epoch= 11 |  2800/2983 batches |  lr=3.61  |  loss  6.14 | ppl   462.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=11 | time: 339.76s | val_loss= 6.82 | valid ppl   917.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 12 |   200/2983 batches |  lr=3.55  |  loss  6.05 | ppl   422.70\n",
      "| epoch= 12 |   400/2983 batches |  lr=3.51  |  loss  6.08 | ppl   436.80\n",
      "| epoch= 12 |   600/2983 batches |  lr=3.48  |  loss  5.98 | ppl   394.78\n",
      "| epoch= 12 |   800/2983 batches |  lr=3.45  |  loss  6.00 | ppl   403.45\n",
      "| epoch= 12 |  1000/2983 batches |  lr=3.41  |  loss  6.05 | ppl   425.11\n",
      "| epoch= 12 |  1200/2983 batches |  lr=3.38  |  loss  6.11 | ppl   450.28\n",
      "| epoch= 12 |  1400/2983 batches |  lr=3.34  |  loss  6.12 | ppl   455.20\n",
      "| epoch= 12 |  1600/2983 batches |  lr=3.30  |  loss  6.17 | ppl   479.77\n",
      "| epoch= 12 |  1800/2983 batches |  lr=3.27  |  loss  6.07 | ppl   432.16\n",
      "| epoch= 12 |  2000/2983 batches |  lr=3.23  |  loss  6.14 | ppl   464.56\n",
      "| epoch= 12 |  2200/2983 batches |  lr=3.20  |  loss  6.03 | ppl   417.33\n",
      "| epoch= 12 |  2400/2983 batches |  lr=3.16  |  loss  6.11 | ppl   451.15\n",
      "| epoch= 12 |  2600/2983 batches |  lr=3.12  |  loss  6.14 | ppl   463.63\n",
      "| epoch= 12 |  2800/2983 batches |  lr=3.09  |  loss  6.08 | ppl   437.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=12 | time: 339.10s | val_loss= 6.39 | valid ppl   598.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 13 |   200/2983 batches |  lr=3.02  |  loss  5.94 | ppl   378.51\n",
      "| epoch= 13 |   400/2983 batches |  lr=2.98  |  loss  5.97 | ppl   391.74\n",
      "| epoch= 13 |   600/2983 batches |  lr=2.94  |  loss  5.84 | ppl   342.90\n",
      "| epoch= 13 |   800/2983 batches |  lr=2.91  |  loss  5.91 | ppl   367.90\n",
      "| epoch= 13 |  1000/2983 batches |  lr=2.87  |  loss  5.95 | ppl   384.60\n",
      "| epoch= 13 |  1200/2983 batches |  lr=2.83  |  loss  6.02 | ppl   411.73\n",
      "| epoch= 13 |  1400/2983 batches |  lr=2.79  |  loss  6.03 | ppl   413.82\n",
      "| epoch= 13 |  1600/2983 batches |  lr=2.76  |  loss  6.11 | ppl   452.25\n",
      "| epoch= 13 |  1800/2983 batches |  lr=2.72  |  loss  6.07 | ppl   430.98\n",
      "| epoch= 13 |  2000/2983 batches |  lr=2.68  |  loss  6.11 | ppl   448.75\n",
      "| epoch= 13 |  2200/2983 batches |  lr=2.64  |  loss  6.03 | ppl   414.78\n",
      "| epoch= 13 |  2400/2983 batches |  lr=2.61  |  loss  6.11 | ppl   451.39\n",
      "| epoch= 13 |  2600/2983 batches |  lr=2.57  |  loss  6.13 | ppl   458.91\n",
      "| epoch= 13 |  2800/2983 batches |  lr=2.53  |  loss  6.06 | ppl   429.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=13 | time: 338.88s | val_loss= 6.33 | valid ppl   563.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 14 |   200/2983 batches |  lr=2.46  |  loss  5.86 | ppl   352.12\n",
      "| epoch= 14 |   400/2983 batches |  lr=2.42  |  loss  5.84 | ppl   344.48\n",
      "| epoch= 14 |   600/2983 batches |  lr=2.38  |  loss  5.72 | ppl   303.58\n",
      "| epoch= 14 |   800/2983 batches |  lr=2.35  |  loss  5.74 | ppl   312.32\n",
      "| epoch= 14 |  1000/2983 batches |  lr=2.31  |  loss  5.79 | ppl   325.40\n",
      "| epoch= 14 |  1200/2983 batches |  lr=2.27  |  loss  5.82 | ppl   337.72\n",
      "| epoch= 14 |  1400/2983 batches |  lr=2.23  |  loss  5.87 | ppl   352.82\n",
      "| epoch= 14 |  1600/2983 batches |  lr=2.20  |  loss  5.92 | ppl   372.88\n",
      "| epoch= 14 |  1800/2983 batches |  lr=2.16  |  loss  5.82 | ppl   335.62\n",
      "| epoch= 14 |  2000/2983 batches |  lr=2.12  |  loss  5.84 | ppl   342.12\n",
      "| epoch= 14 |  2200/2983 batches |  lr=2.09  |  loss  5.73 | ppl   308.38\n",
      "| epoch= 14 |  2400/2983 batches |  lr=2.05  |  loss  5.77 | ppl   321.39\n",
      "| epoch= 14 |  2600/2983 batches |  lr=2.01  |  loss  5.80 | ppl   331.63\n",
      "| epoch= 14 |  2800/2983 batches |  lr=1.97  |  loss  5.77 | ppl   319.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=14 | time: 339.57s | val_loss= 6.02 | valid ppl   411.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 15 |   200/2983 batches |  lr=1.90  |  loss  5.70 | ppl   298.20\n",
      "| epoch= 15 |   400/2983 batches |  lr=1.87  |  loss  5.70 | ppl   298.20\n",
      "| epoch= 15 |   600/2983 batches |  lr=1.83  |  loss  5.57 | ppl   262.29\n",
      "| epoch= 15 |   800/2983 batches |  lr=1.80  |  loss  5.62 | ppl   275.31\n",
      "| epoch= 15 |  1000/2983 batches |  lr=1.76  |  loss  5.65 | ppl   284.60\n",
      "| epoch= 15 |  1200/2983 batches |  lr=1.72  |  loss  5.68 | ppl   294.19\n",
      "| epoch= 15 |  1400/2983 batches |  lr=1.69  |  loss  5.69 | ppl   294.85\n",
      "| epoch= 15 |  1600/2983 batches |  lr=1.65  |  loss  5.72 | ppl   304.24\n",
      "| epoch= 15 |  1800/2983 batches |  lr=1.62  |  loss  5.64 | ppl   280.97\n",
      "| epoch= 15 |  2000/2983 batches |  lr=1.58  |  loss  5.68 | ppl   293.23\n",
      "| epoch= 15 |  2200/2983 batches |  lr=1.55  |  loss  5.58 | ppl   264.94\n",
      "| epoch= 15 |  2400/2983 batches |  lr=1.51  |  loss  5.63 | ppl   279.07\n",
      "| epoch= 15 |  2600/2983 batches |  lr=1.48  |  loss  5.66 | ppl   287.74\n",
      "| epoch= 15 |  2800/2983 batches |  lr=1.44  |  loss  5.62 | ppl   275.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=15 | time: 338.35s | val_loss= 6.17 | valid ppl   476.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 16 |   200/2983 batches |  lr=1.38  |  loss  5.59 | ppl   268.75\n",
      "| epoch= 16 |   400/2983 batches |  lr=1.35  |  loss  5.60 | ppl   269.12\n",
      "| epoch= 16 |   600/2983 batches |  lr=1.31  |  loss  5.46 | ppl   234.32\n",
      "| epoch= 16 |   800/2983 batches |  lr=1.28  |  loss  5.53 | ppl   252.21\n",
      "| epoch= 16 |  1000/2983 batches |  lr=1.25  |  loss  5.56 | ppl   259.38\n",
      "| epoch= 16 |  1200/2983 batches |  lr=1.21  |  loss  5.59 | ppl   267.98\n",
      "| epoch= 16 |  1400/2983 batches |  lr=1.18  |  loss  5.61 | ppl   272.93\n",
      "| epoch= 16 |  1600/2983 batches |  lr=1.15  |  loss  5.65 | ppl   284.38\n",
      "| epoch= 16 |  1800/2983 batches |  lr=1.12  |  loss  5.56 | ppl   259.17\n",
      "| epoch= 16 |  2000/2983 batches |  lr=1.09  |  loss  5.61 | ppl   272.46\n",
      "| epoch= 16 |  2200/2983 batches |  lr=1.06  |  loss  5.49 | ppl   241.25\n",
      "| epoch= 16 |  2400/2983 batches |  lr=1.03  |  loss  5.54 | ppl   253.93\n",
      "| epoch= 16 |  2600/2983 batches |  lr=1.00  |  loss  5.56 | ppl   259.20\n",
      "| epoch= 16 |  2800/2983 batches |  lr=0.97  |  loss  5.50 | ppl   244.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=16 | time: 340.76s | val_loss= 5.80 | valid ppl   330.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 17 |   200/2983 batches |  lr=0.91  |  loss  5.45 | ppl   232.27\n",
      "| epoch= 17 |   400/2983 batches |  lr=0.88  |  loss  5.47 | ppl   236.94\n",
      "| epoch= 17 |   600/2983 batches |  lr=0.85  |  loss  5.33 | ppl   205.70\n",
      "| epoch= 17 |   800/2983 batches |  lr=0.82  |  loss  5.40 | ppl   221.95\n",
      "| epoch= 17 |  1000/2983 batches |  lr=0.80  |  loss  5.40 | ppl   220.82\n",
      "| epoch= 17 |  1200/2983 batches |  lr=0.77  |  loss  5.42 | ppl   226.48\n",
      "| epoch= 17 |  1400/2983 batches |  lr=0.74  |  loss  5.45 | ppl   233.66\n",
      "| epoch= 17 |  1600/2983 batches |  lr=0.72  |  loss  5.49 | ppl   241.32\n",
      "| epoch= 17 |  1800/2983 batches |  lr=0.69  |  loss  5.40 | ppl   221.11\n",
      "| epoch= 17 |  2000/2983 batches |  lr=0.66  |  loss  5.45 | ppl   233.17\n",
      "| epoch= 17 |  2200/2983 batches |  lr=0.64  |  loss  5.34 | ppl   207.82\n",
      "| epoch= 17 |  2400/2983 batches |  lr=0.61  |  loss  5.37 | ppl   215.31\n",
      "| epoch= 17 |  2600/2983 batches |  lr=0.59  |  loss  5.38 | ppl   217.81\n",
      "| epoch= 17 |  2800/2983 batches |  lr=0.57  |  loss  5.34 | ppl   208.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=17 | time: 340.15s | val_loss= 5.76 | valid ppl   315.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 18 |   200/2983 batches |  lr=0.52  |  loss  5.33 | ppl   206.17\n",
      "| epoch= 18 |   400/2983 batches |  lr=0.50  |  loss  5.33 | ppl   205.86\n",
      "| epoch= 18 |   600/2983 batches |  lr=0.48  |  loss  5.19 | ppl   179.31\n",
      "| epoch= 18 |   800/2983 batches |  lr=0.45  |  loss  5.25 | ppl   189.80\n",
      "| epoch= 18 |  1000/2983 batches |  lr=0.43  |  loss  5.27 | ppl   194.00\n",
      "| epoch= 18 |  1200/2983 batches |  lr=0.41  |  loss  5.29 | ppl   197.87\n",
      "| epoch= 18 |  1400/2983 batches |  lr=0.39  |  loss  5.31 | ppl   202.31\n",
      "| epoch= 18 |  1600/2983 batches |  lr=0.37  |  loss  5.33 | ppl   205.83\n",
      "| epoch= 18 |  1800/2983 batches |  lr=0.35  |  loss  5.26 | ppl   191.79\n",
      "| epoch= 18 |  2000/2983 batches |  lr=0.33  |  loss  5.31 | ppl   202.84\n",
      "| epoch= 18 |  2200/2983 batches |  lr=0.31  |  loss  5.19 | ppl   178.86\n",
      "| epoch= 18 |  2400/2983 batches |  lr=0.30  |  loss  5.24 | ppl   188.76\n",
      "| epoch= 18 |  2600/2983 batches |  lr=0.28  |  loss  5.27 | ppl   194.69\n",
      "| epoch= 18 |  2800/2983 batches |  lr=0.26  |  loss  5.22 | ppl   184.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=18 | time: 339.26s | val_loss= 5.63 | valid ppl   277.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 19 |   200/2983 batches |  lr=0.23  |  loss  5.20 | ppl   181.34\n",
      "| epoch= 19 |   400/2983 batches |  lr=0.21  |  loss  5.21 | ppl   182.41\n",
      "| epoch= 19 |   600/2983 batches |  lr=0.20  |  loss  5.08 | ppl   160.35\n",
      "| epoch= 19 |   800/2983 batches |  lr=0.19  |  loss  5.15 | ppl   173.11\n",
      "| epoch= 19 |  1000/2983 batches |  lr=0.17  |  loss  5.16 | ppl   173.39\n",
      "| epoch= 19 |  1200/2983 batches |  lr=0.16  |  loss  5.18 | ppl   178.08\n",
      "| epoch= 19 |  1400/2983 batches |  lr=0.14  |  loss  5.20 | ppl   180.43\n",
      "| epoch= 19 |  1600/2983 batches |  lr=0.13  |  loss  5.23 | ppl   186.08\n",
      "| epoch= 19 |  1800/2983 batches |  lr=0.12  |  loss  5.16 | ppl   173.50\n",
      "| epoch= 19 |  2000/2983 batches |  lr=0.11  |  loss  5.21 | ppl   182.92\n",
      "| epoch= 19 |  2200/2983 batches |  lr=0.10  |  loss  5.10 | ppl   163.91\n",
      "| epoch= 19 |  2400/2983 batches |  lr=0.09  |  loss  5.15 | ppl   172.17\n",
      "| epoch= 19 |  2600/2983 batches |  lr=0.08  |  loss  5.17 | ppl   176.66\n",
      "| epoch= 19 |  2800/2983 batches |  lr=0.07  |  loss  5.13 | ppl   168.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch=19 | time: 338.41s | val_loss= 5.53 | valid ppl   252.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch= 20 |   200/2983 batches |  lr=0.05  |  loss  5.12 | ppl   167.92\n",
      "| epoch= 20 |   400/2983 batches |  lr=0.05  |  loss  5.14 | ppl   170.35\n",
      "| epoch= 20 |   600/2983 batches |  lr=0.04  |  loss  5.01 | ppl   149.99\n",
      "| epoch= 20 |   800/2983 batches |  lr=0.03  |  loss  5.09 | ppl   163.11\n",
      "| epoch= 20 |  1000/2983 batches |  lr=0.03  |  loss  5.11 | ppl   165.87\n",
      "| epoch= 20 |  1200/2983 batches |  lr=0.02  |  loss  5.14 | ppl   169.97\n",
      "| epoch= 20 |  1400/2983 batches |  lr=0.02  |  loss  5.17 | ppl   175.86\n",
      "| epoch= 20 |  1600/2983 batches |  lr=0.01  |  loss  5.19 | ppl   179.49\n",
      "| epoch= 20 |  1800/2983 batches |  lr=0.01  |  loss  5.14 | ppl   169.95\n",
      "| epoch= 20 |  2000/2983 batches |  lr=0.01  |  loss  5.19 | ppl   180.30\n",
      "| epoch= 20 |  2200/2983 batches |  lr=0.00  |  loss  5.08 | ppl   160.51\n",
      "| epoch= 20 |  2400/2983 batches |  lr=0.00  |  loss  5.10 | ppl   163.80\n",
      "| epoch= 20 |  2600/2983 batches |  lr=0.00  |  loss  5.15 | ppl   173.01\n",
      "| epoch= 20 |  2800/2983 batches |  lr=0.00  |  loss  5.09 | ppl   163.08\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to step 59662 times. The specified number of total steps is 59660",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-3c380b62f195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_epochs(optimizer=optimizer,\n\u001b[0m\u001b[1;32m      2\u001b[0m              \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m              \u001b[0mn_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              log_interval=200)\n",
      "\u001b[0;32m<ipython-input-98-cdefd8ed5843>\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(optimizer, scheduler, n_seq, ntokens, epochs, log_interval)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH_DEPRECATION_WARNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mget_lr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep_num\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m             raise ValueError(\"Tried to step {} times. The specified number of total steps is {}\"\n\u001b[0m\u001b[1;32m   1213\u001b[0m                              .format(step_num + 1, self.total_steps))\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tried to step 59662 times. The specified number of total steps is 59660"
     ]
    }
   ],
   "source": [
    "train_epochs(optimizer=optimizer,\n",
    "             scheduler=scheduler,\n",
    "             n_seq=n_seq,\n",
    "             epochs=20,\n",
    "             log_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T21:37:30.691991Z",
     "start_time": "2020-06-24T21:37:30.644761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open('model_val_loss_5.01_val_ppl_149.39.pt', 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    # Currently, only rnn model supports flatten_parameters function.\n",
    "    model.rnn.flatten_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T21:37:32.617560Z",
     "start_time": "2020-06-24T21:37:30.983096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | val_loss= 5.01 | val ppl   149.39\n"
     ]
    }
   ],
   "source": [
    "# Run on validation data.\n",
    "val_loss = evaluate(val_data)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | {val_loss=:5.2f} | val ppl {math.exp(val_loss):8.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T21:37:32.617560Z",
     "start_time": "2020-06-24T21:37:30.983096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test_loss= 4.94 | test ppl   140.26\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | {test_loss=:5.2f} | test ppl {math.exp(test_loss):8.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(\n",
    "#     torch.load(\n",
    "#     model_data_filepath + 'word_language_model_quantize.pth',\n",
    "#         map_location = torch.device('cpu')\n",
    "#     )\n",
    "# )\n",
    "# model.eval()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
