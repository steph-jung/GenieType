{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\">Objective</a></span></li><li><span><a href=\"#Data-Models\" data-toc-modified-id=\"Data-Models-2\">Data Models</a></span></li><li><span><a href=\"#Datasets\" data-toc-modified-id=\"Datasets-3\">Datasets</a></span></li><li><span><a href=\"#GRU\" data-toc-modified-id=\"GRU-4\">GRU</a></span></li><li><span><a href=\"#Training-Functions\" data-toc-modified-id=\"Training-Functions-5\">Training Functions</a></span></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-6\">Train Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-DataLoaders\" data-toc-modified-id=\"Prepare-DataLoaders-6.1\">Prepare DataLoaders</a></span></li><li><span><a href=\"#Training-Loop\" data-toc-modified-id=\"Training-Loop-6.2\">Training Loop</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:30.912108Z",
     "start_time": "2020-06-24T08:33:30.610960Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:30.928987Z",
     "start_time": "2020-06-24T08:33:30.914451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdd0530daf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The goal of this notebook is to train a language model from scratch on `wikitext-2`, which you can find [here](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). Our focus will be on getting the pre-processing and training loops working in the traditional, non-federated setting. In a separate notebook we'll do the same thing for the federated setting, which you can read more about in [this](https://arxiv.org/pdf/1811.03604.pdf) paper (which we'll refer to as the `Google Smart Keyboard Paper`).\n",
    "\n",
    "This notebook borrows heavily from [this](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html) pytorch tutorial, which is absolutely outstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Models\n",
    "\n",
    "Let's create data models for a corpus of text. The `Google Smart Keyboard Paper` uses a vocabulary of size `10,000` and inlcudes tokens for the beggining of sentence, end of setence, and out-of-vocab words. During inference time the probabilities for these tokens are ignored.\n",
    "\n",
    "TODO: \n",
    "\n",
    "* Limit vocab size\n",
    "* Add `<BOS>` tokens?\n",
    "* Sort words by descending frequency to enable training with `AdaptiveLogSoftMax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:30.942127Z",
     "start_time": "2020-06-24T08:33:30.935021Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DescendingDict:\n",
    "    \"\"\"Model a vocab as a mapping word <-> index, ordered by descending frequency.\"\"\"\n",
    "    \n",
    "    def __init__(self, words, vocab_sz=None):\n",
    "        word2freq = Counter(words).most_common(vocab_sz)\n",
    "        self.idx2word = [tup[0] for tup in word2freq]\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.idx2word)}\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "                                \n",
    "    def get_index(self, word):\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def get_indices(self, words):\n",
    "        return [self.word2idx[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:30.953768Z",
     "start_time": "2020-06-24T08:33:30.944239Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class WikiCorpus:\n",
    "    \"\"\"Encode a corpus of text already processed in the wikitext style.\"\"\"\n",
    "    \n",
    "    def __init__(self, dirpath, vocab_sz=None):\n",
    "        \"\"\"Build a corpus given a dir with train, valid, and test .txt files.\"\"\"\n",
    "        words = self.tokenize(os.path.join(dirpath, \"train.txt\"))\n",
    "        self.dictionary = DescendingDict(words, vocab_sz)\n",
    "        self.train = self.vectorize(os.path.join(dirpath, \"train.txt\"))\n",
    "        self.valid = self.vectorize(os.path.join(dirpath, \"valid.txt\"))\n",
    "        self.test = self.vectorize(os.path.join(dirpath, \"test.txt\"))\n",
    "        \n",
    "    def tokenize(self, fpath):\n",
    "        \"\"\"Split on new lines and append <eos> tokens.\"\"\"\n",
    "        words = []\n",
    "        with open(fpath, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words += line.split() + [\"<eos>\"]\n",
    "        return words\n",
    "        \n",
    "    def vectorize(self, fpath):\n",
    "        \"\"\"Return a tensor of indexes encoding the words in a file.\"\"\"\n",
    "        idxs = []\n",
    "        with open(fpath, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + [\"<eos>\"]\n",
    "                idxs.extend(self.dictionary.get_indices(words))\n",
    "        return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:30.963356Z",
     "start_time": "2020-06-24T08:33:30.956142Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    \"\"\"Dataset for language models.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, n_sequences, device):\n",
    "        \"\"\"Partition text data and move to device.\"\"\"\n",
    "        self.data = self.partition(data, n_sequences, device)\n",
    "        self.n_sequences = n_sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Return the i-th element and its target.\"\"\"\n",
    "        if i == len(self.data) - 1:\n",
    "            return self.data[i-1], self.data[i]\n",
    "        return self.data[i], self.data[i + 1]\n",
    "        \n",
    "    def partition(self, data, n_sequences, device):\n",
    "        \"\"\"Re-shape data to have ``n_sequences`` columns (discards remainder).\"\"\"\n",
    "        n_rows = len(data) // n_sequences\n",
    "        data = data[:n_rows * n_sequences]\n",
    "        data = data.view(n_sequences, -1).t().contiguous()  #TODO: get rid of .t()\n",
    "        return data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU \n",
    "\n",
    "We'll use a single layer `GRU` with somewhere around `600` hidden units. We'll tie the embedding weights to the softmax layer, as described in [this](https://arxiv.org/pdf/1611.01462.pdf) paper.\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Variational dropout\n",
    "* Weight dropping\n",
    "* Ignore `<unk>`, `<eos>` at inference time\n",
    "* Neural cache\n",
    "* Simplify `reapackage_hidden` procedure\n",
    "* Support multiple layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:30.974795Z",
     "start_time": "2020-06-24T08:33:30.965232Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LModel(nn.Module):\n",
    "    \"\"\"Abstract base class for language models with a recurrent unit and encoder, decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_sz, emb_dim, hid_dim, dropout, tie_weights=False, layer_norm=False):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim)\n",
    "        self.decoder = nn.Linear(hid_dim, vocab_sz)\n",
    "        \n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.init_weights()\n",
    "        if tie_weights:\n",
    "            assert hid_dim == emb_dim, f\"{hid_dim= } must match {emb_dim= }!\"\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "        \n",
    "        self.layer_norm = layer_norm   \n",
    "        if layer_norm:\n",
    "            self.lnorm = nn.LayerNorm(hid_dim)\n",
    "            \n",
    "    def init_weights(self, k=0.1):\n",
    "        \"\"\"Initialise weights from a uniform distribution U(-k, k).\"\"\"\n",
    "        self.encoder.weight.data.uniform_(-k, k)\n",
    "        self.decoder.weight.data.uniform_(-k, k)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        \n",
    "    def init_hidden(self, n_sequences):\n",
    "        \"\"\"Initialise hidden weights.\"\"\"\n",
    "        return torch.zeros(1, n_sequences, self.hid_dim, requires_grad=False)\n",
    "    \n",
    "    def repackage_hidden(self, hidden):\n",
    "        \"\"\"Return hidden states in new tensors detatched from their history.\"\"\"\n",
    "        if isinstance(hidden, torch.Tensor):\n",
    "            return hidden.detach()\n",
    "        return tuple(self.repackage_hidden(v) for v in hidden)\n",
    "    \n",
    "    def save(self, fpath):\n",
    "        \"\"\"Serialise weights to disc.\"\"\"\n",
    "        torch.save(self.state_dict(), fpath)\n",
    "        \n",
    "    def load(self, fpath):\n",
    "        \"\"\"Load pre-trained weights.\"\"\"\n",
    "        self.load_state_dict(torch.load(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:30.983047Z",
     "start_time": "2020-06-24T08:33:30.976394Z"
    }
   },
   "outputs": [],
   "source": [
    "class GRULModel(LModel):\n",
    "    \"\"\"Language model to be trained with traditional softmax.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_sz, emb_dim, hid_dim, dropout, tie_weights=False, layer_norm=False):\n",
    "        super().__init__(vocab_sz, emb_dim, hid_dim, dropout, tie_weights, layer_norm)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.gru(emb, hidden)\n",
    "        if self.layer_norm:\n",
    "            output = self.lnorm(output)\n",
    "            hidden = self.lnorm(hidden)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.vocab_sz)\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def predict(self, x, ignore=None):\n",
    "        \"\"\"Return the most likely next word, ignoring certain tokens.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:30.996693Z",
     "start_time": "2020-06-24T08:33:30.986757Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdaptiveGRULModel(LModel):\n",
    "    \"\"\"Language model to be trained with adaptive softmax.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_sz, emb_dim, hid_dim, dropout, layer_norm=False):\n",
    "        super().__init__(vocab_sz, emb_dim, hid_dim, dropout, tie_weights=True, layer_norm=layer_norm)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.gru(emb, hidden)\n",
    "        output = output.view(-1, self.hid_dim)\n",
    "        if self.layer_norm:\n",
    "            output = self.lnorm(output)\n",
    "            hidden = self.lnorm(hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the next word, ignoring <unk> and <eos>.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainers\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Save losses, timestamps to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:31.005697Z",
     "start_time": "2020-06-24T08:33:30.998878Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Abstract base class for training a general neural model.\"\"\"\n",
    "    \n",
    "    def __init__(self, train_dl, val_dl, model, criterion, optimizer, scheduler):\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def _descend(self, loss, max_norm=None):\n",
    "        \"\"\"Perform one step of gradient descent.\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if max_norm:\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:31.022821Z",
     "start_time": "2020-06-24T08:33:31.010668Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LMTrainer(Trainer):\n",
    "    \"\"\"Container for training a language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, train_dl, val_dl, model, criterion, optimizer, scheduler):\n",
    "        \"\"\"Initialise trainer with traditional or adaptive softmax.\"\"\"\n",
    "        super().__init__(train_dl, val_dl, model, criterion, optimizer, scheduler)\n",
    "        self.losses = []\n",
    "        self.timestamps = []\n",
    "        \n",
    "    def _compute_loss(self, output, targets):\n",
    "        \"\"\"Compute traditional or adaptive softmax.\"\"\"\n",
    "        if isinstance(self.criterion, nn.AdaptiveLogSoftmaxWithLoss):\n",
    "            return self.criterion(output, targets)[1]\n",
    "        return self.criterion(output, targets)\n",
    "                        \n",
    "    def _train(self, hidden, max_norm):\n",
    "        \"\"\"Train for a single epoch and return the loss.\"\"\"\n",
    "        loss, N = 0, 0\n",
    "        for data, targets in self.train_dl:\n",
    "            hidden = self.model.repackage_hidden(hidden)\n",
    "            output, hidden = self.model(data, hidden)\n",
    "            batch_loss = self._compute_loss(output, targets)\n",
    "            self._descend(batch_loss, max_norm)\n",
    "            loss += batch_loss.item() * len(data)\n",
    "            N += len(data)\n",
    "        return loss / N\n",
    "        \n",
    "    #TODO: why does the tutorial call init_hidden() inside each epoch?\n",
    "    def train(self, n_epochs, max_norm, save_path):\n",
    "        \"\"\"Train a language model with optional graident clipping.\"\"\"\n",
    "        start = time.time()\n",
    "        best_val_loss, best_val_ppl = None, None\n",
    "        hidden = self.model.init_hidden(self.train_dl.dataset.n_sequences)\n",
    "\n",
    "        for epoch in range(n_epochs): \n",
    "            self.model.train()\n",
    "            train_loss = self._train(hidden, max_norm)  \n",
    "            scheduler.step()\n",
    "\n",
    "            elapsed = (time.time() - start) / 60\n",
    "            self.timestamps.append(elapsed)\n",
    "            self.losses.append(train_loss)\n",
    "\n",
    "            val_loss, val_ppl = self.evaluate(self.val_dl)\n",
    "            print(f\"{epoch= :3d} | {elapsed= :4.2f}min | {train_loss= :5.2f} | \"\n",
    "                  f\"{val_loss= :5.2f} | {val_ppl= :8.2f}\")\n",
    "\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_val_loss_ppl = val_ppl\n",
    "                model.save(save_path)\n",
    "        os.rename(save_path, f\"{save_path}_{best_val_loss:.2f}_loss_{best_val_loss_ppl:.2f}_ppl.pt\")\n",
    "        self.log_results(f\"{save_path}_{best_val_loss:.2f}_loss_{best_val_loss_ppl:.2f}_ppl_log.json\")\n",
    "        \n",
    "    def evaluate(self, dl):\n",
    "        \"\"\"Return loss and perplexity.\"\"\"\n",
    "        self.model.eval()\n",
    "        loss = 0\n",
    "        hidden = self.model.init_hidden(dl.dataset.n_sequences)\n",
    "\n",
    "        for data, targets in dl:\n",
    "            output, hidden = self.model(data, hidden)\n",
    "            batch_loss = self._compute_loss(output, targets)\n",
    "            loss += batch_loss\n",
    "        return loss.item(), loss.exp().item()\n",
    "    \n",
    "    def log_results(self, fpath):\n",
    "        \"\"\"Write losses and training times to disc.\"\"\"\n",
    "        results = {\"losses\": self.losses, \"timestamps\": self.timestamps}\n",
    "        with open(fpath, \"w\") as f:\n",
    "            json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "TODO:\n",
    "\n",
    "* ASGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:31.030082Z",
     "start_time": "2020-06-24T08:33:31.024507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:32.168434Z",
     "start_time": "2020-06-24T08:33:31.033277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "dirpath = Path(\"../data/wikitext-2/\")\n",
    "corpus = WikiCorpus(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:32.174242Z",
     "start_time": "2020-06-24T08:33:32.170434Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Return data as a 2D-tensor and targets as a 1D tensor.\"\"\"\n",
    "    data = torch.stack([tup[0] for tup in batch])\n",
    "    targets = torch.cat([tup[1] for tup in batch])\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:32.184113Z",
     "start_time": "2020-06-24T08:33:32.177827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prep Datasets\n",
    "n_sequences = 50\n",
    "train_ds = LMDataset(corpus.train[:35*50], n_sequences, device)\n",
    "val_ds = LMDataset(corpus.valid[:35*50], n_sequences, device)\n",
    "test_ds = LMDataset(corpus.test[:35*50], n_sequences, device)\n",
    "\n",
    "# Prep DataLoaders\n",
    "batch_sz = 64\n",
    "train_dl = DataLoader(train_ds, batch_sz, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_sz, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(test_ds, batch_sz, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:32.277616Z",
     "start_time": "2020-06-24T08:33:32.187258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set model params\n",
    "vocab_sz = len(corpus.dictionary)\n",
    "dropout = 0.3\n",
    "emb_dim = 96\n",
    "hidden_dim = emb_dim\n",
    "\n",
    "# Initialise model\n",
    "model = GRULModel(vocab_sz, emb_dim, hidden_dim, dropout=dropout, tie_weights=True, layer_norm=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:33:32.286688Z",
     "start_time": "2020-06-24T08:33:32.279113Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to save model to\n",
    "save_path = Path(\"../models/lmodel\")\n",
    "\n",
    "# Set training & schedluer params\n",
    "max_lr = 1\n",
    "max_norm = 0.25\n",
    "n_epochs = 40\n",
    "steps_per_epoch = len(train_dl)\n",
    "\n",
    "# Choose criterion\n",
    "# cutoffs = [2000, 10_000]\n",
    "# criterion = nn.AdaptiveLogSoftmaxWithLoss(hidden_dim, vocab_sz, cutoffs=cutoffs)\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# Initialize optimizer & scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=1, momentum=0.9, nesterov=True)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=n_epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = LMTrainer(train_dl, val_dl, model, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:34:07.575593Z",
     "start_time": "2020-06-24T08:33:32.289204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=   0 | elapsed= 0.01min | train_loss= 10.69 | val_loss= 10.66 | val_ppl= 42756.83\n",
      "epoch=   1 | elapsed= 0.03min | train_loss= 10.66 | val_loss= 10.61 | val_ppl= 40511.17\n",
      "epoch=   2 | elapsed= 0.04min | train_loss= 10.60 | val_loss= 10.47 | val_ppl= 35408.62\n",
      "epoch=   3 | elapsed= 0.06min | train_loss= 10.44 | val_loss= 10.21 | val_ppl= 27307.49\n",
      "epoch=   4 | elapsed= 0.07min | train_loss= 10.15 | val_loss=  9.88 | val_ppl= 19610.41\n",
      "epoch=   5 | elapsed= 0.09min | train_loss=  9.78 | val_loss=  9.53 | val_ppl= 13821.92\n",
      "epoch=   6 | elapsed= 0.10min | train_loss=  9.38 | val_loss=  9.16 | val_ppl=  9526.23\n",
      "epoch=   7 | elapsed= 0.12min | train_loss=  8.95 | val_loss=  8.97 | val_ppl=  7836.64\n",
      "epoch=   8 | elapsed= 0.13min | train_loss=  8.70 | val_loss=  8.63 | val_ppl=  5605.45\n",
      "epoch=   9 | elapsed= 0.15min | train_loss=  8.33 | val_loss=  8.42 | val_ppl=  4547.23\n",
      "epoch=  10 | elapsed= 0.17min | train_loss=  8.06 | val_loss=  8.35 | val_ppl=  4243.54\n",
      "epoch=  11 | elapsed= 0.18min | train_loss=  7.92 | val_loss=  8.49 | val_ppl=  4867.73\n",
      "epoch=  12 | elapsed= 0.20min | train_loss=  7.91 | val_loss=  8.25 | val_ppl=  3834.52\n",
      "epoch=  13 | elapsed= 0.21min | train_loss=  7.67 | val_loss=  8.25 | val_ppl=  3820.76\n",
      "epoch=  14 | elapsed= 0.23min | train_loss=  7.55 | val_loss=  8.17 | val_ppl=  3538.83\n",
      "epoch=  15 | elapsed= 0.24min | train_loss=  7.47 | val_loss=  8.36 | val_ppl=  4263.75\n",
      "epoch=  16 | elapsed= 0.26min | train_loss=  7.52 | val_loss=  8.06 | val_ppl=  3153.61\n",
      "epoch=  17 | elapsed= 0.27min | train_loss=  7.23 | val_loss=  8.24 | val_ppl=  3771.03\n",
      "epoch=  18 | elapsed= 0.29min | train_loss=  7.29 | val_loss=  7.98 | val_ppl=  2935.38\n",
      "epoch=  19 | elapsed= 0.30min | train_loss=  7.05 | val_loss=  8.16 | val_ppl=  3494.66\n",
      "epoch=  20 | elapsed= 0.32min | train_loss=  7.11 | val_loss=  7.95 | val_ppl=  2842.53\n",
      "epoch=  21 | elapsed= 0.33min | train_loss=  6.91 | val_loss=  8.04 | val_ppl=  3095.73\n",
      "epoch=  22 | elapsed= 0.35min | train_loss=  6.89 | val_loss=  7.95 | val_ppl=  2846.22\n",
      "epoch=  23 | elapsed= 0.36min | train_loss=  6.80 | val_loss=  7.97 | val_ppl=  2901.24\n",
      "epoch=  24 | elapsed= 0.38min | train_loss=  6.73 | val_loss=  7.95 | val_ppl=  2827.60\n",
      "epoch=  25 | elapsed= 0.39min | train_loss=  6.69 | val_loss=  7.95 | val_ppl=  2838.67\n",
      "epoch=  26 | elapsed= 0.40min | train_loss=  6.61 | val_loss=  7.94 | val_ppl=  2799.93\n",
      "epoch=  27 | elapsed= 0.42min | train_loss=  6.57 | val_loss=  7.95 | val_ppl=  2833.18\n",
      "epoch=  28 | elapsed= 0.43min | train_loss=  6.52 | val_loss=  7.92 | val_ppl=  2756.48\n",
      "epoch=  29 | elapsed= 0.45min | train_loss=  6.48 | val_loss=  7.93 | val_ppl=  2781.40\n",
      "epoch=  30 | elapsed= 0.46min | train_loss=  6.43 | val_loss=  7.92 | val_ppl=  2739.35\n",
      "epoch=  31 | elapsed= 0.47min | train_loss=  6.40 | val_loss=  7.92 | val_ppl=  2758.10\n",
      "epoch=  32 | elapsed= 0.49min | train_loss=  6.36 | val_loss=  7.91 | val_ppl=  2732.05\n",
      "epoch=  33 | elapsed= 0.50min | train_loss=  6.33 | val_loss=  7.92 | val_ppl=  2741.68\n",
      "epoch=  34 | elapsed= 0.52min | train_loss=  6.31 | val_loss=  7.92 | val_ppl=  2738.16\n",
      "epoch=  35 | elapsed= 0.53min | train_loss=  6.29 | val_loss=  7.91 | val_ppl=  2737.25\n",
      "epoch=  36 | elapsed= 0.54min | train_loss=  6.27 | val_loss=  7.91 | val_ppl=  2737.14\n",
      "epoch=  37 | elapsed= 0.56min | train_loss=  6.27 | val_loss=  7.91 | val_ppl=  2737.18\n",
      "epoch=  38 | elapsed= 0.57min | train_loss=  6.26 | val_loss=  7.91 | val_ppl=  2737.16\n",
      "epoch=  39 | elapsed= 0.58min | train_loss=  6.26 | val_loss=  7.91 | val_ppl=  2737.16\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train(n_epochs, max_norm, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T08:34:07.837749Z",
     "start_time": "2020-06-24T08:34:07.579419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.548727512359619, 1898.3255615234375)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
