{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\">Objective</a></span></li><li><span><a href=\"#Data-Models\" data-toc-modified-id=\"Data-Models-2\">Data Models</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T23:18:13.892635Z",
     "start_time": "2020-06-21T23:18:13.884013Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The goal of this notebook is to train a language model from scratch on `wikitext-2`, which you can find [here](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). Our focus will be on getting the pre-processing and training loops working in the traditional, non-federated setting.  \n",
    "\n",
    "This notebook borrows heavily from [this](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html) pytorch tutorial, which is absolutely outstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T23:51:09.691869Z",
     "start_time": "2020-06-21T23:51:09.681194Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    \"\"\"Base class for encoding a vocabulary.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a new word to the dictionary.\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "                        \n",
    "    def get_index(self, word):\n",
    "        \"\"\"Return the index of a word.\"\"\"\n",
    "        return self.word2idx[word]\n",
    "            \n",
    "    def add_words(self, words):\n",
    "        \"\"\"Add a list of words to the dictionary.\"\"\"\n",
    "        for word in words:\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def get_indexes(self, words):\n",
    "        \"\"\"Return the indexes of a list of words.\"\"\"\n",
    "        return [self.get_index(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T23:53:50.985924Z",
     "start_time": "2020-06-21T23:53:50.976199Z"
    }
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \"\"\"Base class for modeling a corpus of text.\"\"\"\n",
    "    \n",
    "    def __init__(self, dirpath):\n",
    "        \"\"\"Initialise a corpus given a dir with train, valid, and test .txt files.\"\"\"\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(dirpath, \"train.txt\"))\n",
    "        self.valid = self.tokenize(os.path.join(dirpath, \"valid.txt\"))\n",
    "        self.test = self.test(os.path.join(dirpath, \"test.txt\"))\n",
    "        \n",
    "    def vectorize(self, fpath):\n",
    "        \"\"\"Return a tensor of indexes encoding the words in a file.\"\"\"\n",
    "        idxs = []\n",
    "        with open(fpath, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = f.split().append(\"<EOS>\")\n",
    "                self.dictionary.add_words(words)\n",
    "                idxs.extend(self.dictionary.get_indexes(words))\n",
    "        return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
