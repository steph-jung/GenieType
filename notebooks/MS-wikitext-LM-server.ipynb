{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\">Objective</a></span></li><li><span><a href=\"#Data-Models\" data-toc-modified-id=\"Data-Models-2\">Data Models</a></span></li><li><span><a href=\"#Datasets\" data-toc-modified-id=\"Datasets-3\">Datasets</a></span></li><li><span><a href=\"#GRU\" data-toc-modified-id=\"GRU-4\">GRU</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T02:52:25.371498Z",
     "start_time": "2020-06-22T02:52:25.363245Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The goal of this notebook is to train a language model from scratch on `wikitext-2`, which you can find [here](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). Our focus will be on getting the pre-processing and training loops working in the traditional, non-federated setting. In a separate notebook we'll do the same thing for the federated setting, which you can read more about in [this](https://arxiv.org/pdf/1811.03604.pdf) paper (which we'll refer to as the `Google Smart Keyboard Paper`).\n",
    "\n",
    "This notebook borrows heavily from [this](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html) pytorch tutorial, which is absolutely outstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Models\n",
    "\n",
    "Let's create data models for a corpus of text. The `Google Smart Keyboard Paper` uses a vocabulary of size `10,000` and inlcudes tokens for the beggining of sentence, end of setence, and out-of-vocab words. During inference time the probabilities for these tokens are ignored.\n",
    "\n",
    "TODO: \n",
    "\n",
    "* Limit vocab size\n",
    "* Add `<BOS>` tokens\n",
    "* Sort words by descending frequency to enable training with `AdaptiveLogSoftMax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T00:37:58.607740Z",
     "start_time": "2020-06-22T00:37:58.600222Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    \"\"\"Base class for encoding a vocabulary.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a new word to the dictionary.\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "                        \n",
    "    def get_index(self, word):\n",
    "        \"\"\"Return the index of a word.\"\"\"\n",
    "        return self.word2idx[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T00:38:10.269261Z",
     "start_time": "2020-06-22T00:38:10.258579Z"
    }
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \"\"\"Base class for encoding a corpus of text.\"\"\"\n",
    "    \n",
    "    def __init__(self, dirpath):\n",
    "        \"\"\"Initialise a corpus given a dir with train, valid, and test .txt files.\"\"\"\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(dirpath, \"train.txt\"))\n",
    "        self.valid = self.tokenize(os.path.join(dirpath, \"valid.txt\"))\n",
    "        self.test = self.test(os.path.join(dirpath, \"test.txt\"))\n",
    "        \n",
    "    def vectorize(self, fpath):\n",
    "        \"\"\"Return a tensor of indexes encoding the words in a file.\"\"\"\n",
    "        idxs = []\n",
    "        with open(fpath, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = f.split().append(\"<EOS>\")\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                    idxs.append(self.dictionary.get_index(word))\n",
    "        return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU \n",
    "\n",
    "We'll use a single layer `GRU` with somewhere around `600` hidden units. We'll tie the embedding weights to the softmax layer, as described in [this](https://arxiv.org/pdf/1611.01462.pdf) paper.\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Add Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T01:01:12.158789Z",
     "start_time": "2020-06-22T01:01:12.142837Z"
    }
   },
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, recurrent module, and a decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers=1, dropout=0.5, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, n_layers)\n",
    "        self.decoder = nn.Linear(hid_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        if tie_weights:\n",
    "            assert hid_dim == emb_dim, f\"{hid_dim= } must match {emb_dim= }!\"\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "    \n",
    "    #TODO: why do we initialise the bias with zeros?\n",
    "    def init_weights(self, k=0.1):\n",
    "        \"\"\"Initialise weights from a uniform distribution U(-k, k).\"\"\"\n",
    "        self.encoder.weight.data.uniform_(-k, k)\n",
    "        self.decoder.weight.data.uniform_(-k, k)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        \n",
    "    #TODO: why do we return a tuple?\n",
    "    def init_hidden(self, batch_sz):\n",
    "        \"\"\"Initialise hidden weights.\"\"\"\n",
    "        weights = next(self.parameters())\n",
    "        return (weight.new_zeros(self.n_layers, batch_sz, self.hid_dim),\n",
    "                weight.new_zeros(self.n_layers, batch_sz, self.hid_dim))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.gru(emb, hidden)\n",
    "        decoded = self.decoder(self.drop(output))\n",
    "        return decoded, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
