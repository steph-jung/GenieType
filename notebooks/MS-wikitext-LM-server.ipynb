{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\">Objective</a></span></li><li><span><a href=\"#Data-Models\" data-toc-modified-id=\"Data-Models-2\">Data Models</a></span></li><li><span><a href=\"#Datasets\" data-toc-modified-id=\"Datasets-3\">Datasets</a></span></li><li><span><a href=\"#GRU\" data-toc-modified-id=\"GRU-4\">GRU</a></span></li><li><span><a href=\"#Training-Functions\" data-toc-modified-id=\"Training-Functions-5\">Training Functions</a></span></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-6\">Train Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-DataLoaders\" data-toc-modified-id=\"Prepare-DataLoaders-6.1\">Prepare DataLoaders</a></span></li><li><span><a href=\"#Training-Loop\" data-toc-modified-id=\"Training-Loop-6.2\">Training Loop</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:03.909968Z",
     "start_time": "2020-06-23T17:50:03.902165Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:03.938693Z",
     "start_time": "2020-06-23T17:50:03.933120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f61dc08d370>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The goal of this notebook is to train a language model from scratch on `wikitext-2`, which you can find [here](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). Our focus will be on getting the pre-processing and training loops working in the traditional, non-federated setting. In a separate notebook we'll do the same thing for the federated setting, which you can read more about in [this](https://arxiv.org/pdf/1811.03604.pdf) paper (which we'll refer to as the `Google Smart Keyboard Paper`).\n",
    "\n",
    "This notebook borrows heavily from [this](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html) pytorch tutorial, which is absolutely outstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Models\n",
    "\n",
    "Let's create data models for a corpus of text. The `Google Smart Keyboard Paper` uses a vocabulary of size `10,000` and inlcudes tokens for the beggining of sentence, end of setence, and out-of-vocab words. During inference time the probabilities for these tokens are ignored.\n",
    "\n",
    "TODO: \n",
    "\n",
    "* Limit vocab size\n",
    "* Add `<BOS>` tokens?\n",
    "* Sort words by descending frequency to enable training with `AdaptiveLogSoftMax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:03.945938Z",
     "start_time": "2020-06-23T17:50:03.940548Z"
    }
   },
   "outputs": [],
   "source": [
    "class DescendingDict:\n",
    "    \"\"\"Model a vocab as a mapping word <-> index, ordered by descending frequency.\"\"\"\n",
    "    \n",
    "    def __init__(self, words, vocab_size=None):\n",
    "        word2freq = Counter(words).most_common(vocab_size)\n",
    "        self.idx2word = [tup[0] for tup in word2freq]\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.idx2word)}\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "                                \n",
    "    def get_index(self, word):\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def get_indices(self, words):\n",
    "        return [self.word2idx[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:03.958925Z",
     "start_time": "2020-06-23T17:50:03.950021Z"
    }
   },
   "outputs": [],
   "source": [
    "class WikiCorpus:\n",
    "    \"\"\"Encode a corpus of text already processed in the wikitext style.\"\"\"\n",
    "    \n",
    "    def __init__(self, dirpath, vocab_size=None):\n",
    "        \"\"\"Build a corpus given a dir with train, valid, and test .txt files.\"\"\"\n",
    "        words = self.tokenize(os.path.join(dirpath, \"train.txt\"))\n",
    "        self.dictionary = DescendingDict(words, vocab_size)\n",
    "        self.train = self.vectorize(os.path.join(dirpath, \"train.txt\"))\n",
    "        self.valid = self.vectorize(os.path.join(dirpath, \"valid.txt\"))\n",
    "        self.test = self.vectorize(os.path.join(dirpath, \"test.txt\"))\n",
    "        \n",
    "    def tokenize(self, fpath):\n",
    "        \"\"\"Split on new lines and append <eos> tokens.\"\"\"\n",
    "        words = []\n",
    "        with open(fpath, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words += line.split() + [\"<eos>\"]\n",
    "        return words\n",
    "        \n",
    "    def vectorize(self, fpath):\n",
    "        \"\"\"Return a tensor of indexes encoding the words in a file.\"\"\"\n",
    "        idxs = []\n",
    "        with open(fpath, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + [\"<eos>\"]\n",
    "                idxs.extend(self.dictionary.get_indices(words))\n",
    "        return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:03.971572Z",
     "start_time": "2020-06-23T17:50:03.961849Z"
    }
   },
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    \"\"\"Dataset for language models.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, n_partitions, device):\n",
    "        \"\"\"Partition text data into sequences and move to device.\"\"\"\n",
    "        self.data = self.partition(data, n_partitions, device)\n",
    "        self.n_sequences = n_sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Return the i-th element from all sequences along with their targets.\"\"\"\n",
    "        if i == len(self.data) - 1:\n",
    "            return self.data[i-1], self.data[i]\n",
    "        return self.data[i], self.data[i + 1]\n",
    "        \n",
    "    def partition(self, data, n_partitions, device):\n",
    "        \"\"\"Re-shape data to have ``n_partitions`` columns (discards remainder).\"\"\"\n",
    "        n_rows = len(data) // n_partitions\n",
    "        data = data[:n_rows * n_partitions]\n",
    "        data = data.view(n_partitions, -1).t().contiguous()\n",
    "        return data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU \n",
    "\n",
    "We'll use a single layer `GRU` with somewhere around `600` hidden units. We'll tie the embedding weights to the softmax layer, as described in [this](https://arxiv.org/pdf/1611.01462.pdf) paper.\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Add Layer Normalization\n",
    "* Train with and without weight tying\n",
    "* Variational dropout\n",
    "* Weight dropping\n",
    "* Gradient clipping\n",
    "* Ignore `<unk>`, `<eos>` at inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:03.993718Z",
     "start_time": "2020-06-23T17:50:03.975582Z"
    }
   },
   "outputs": [],
   "source": [
    "class GRULModel(nn.Module):\n",
    "    \"\"\"Language model with an encoder, GRU module, and a decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers=1, dropout=0.5, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, n_layers)\n",
    "        self.decoder = nn.Linear(hid_dim, vocab_size)\n",
    "                \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.init_weights()\n",
    "        if tie_weights:\n",
    "            assert hid_dim == emb_dim, f\"{hid_dim= } must match {emb_dim= }!\"\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "    \n",
    "    #TODO: why do we initialise with zero bias?\n",
    "    def init_weights(self, k=0.1):\n",
    "        \"\"\"Initialise weights from a uniform distribution U(-k, k).\"\"\"\n",
    "        self.encoder.weight.data.uniform_(-k, k)\n",
    "        self.decoder.weight.data.uniform_(-k, k)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialise hidden weights.\"\"\"\n",
    "        weights = next(self.parameters())\n",
    "        return weights.new_zeros(self.n_layers, batch_size, self.hid_dim)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.gru(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.vocab_size)\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def predict(self, x, ignore=None):\n",
    "        \"\"\"Return the most likely next word, ignoring certain tokens.\"\"\"\n",
    "        emb = self.encoder(x)\n",
    "        output, hidden = self.gru(emb)\n",
    "        decoded = self.decoder(output).view(-1, self.vocab_size)\n",
    "        return decoded.argmax(dim=0)   \n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Serialise weights to disc.\"\"\"\n",
    "        torch.save(self.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Refactor to a class `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:04.001604Z",
     "start_time": "2020-06-23T17:50:03.996680Z"
    }
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Return hidden states in new tensors detatched from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:04.009955Z",
     "start_time": "2020-06-23T17:50:04.003267Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_val_metrics(model, val_dl):\n",
    "    \"\"\"Return validation loss and perplexity.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    hidden = model.init_hidden(val_dl.dataset.n_sequences)\n",
    "    \n",
    "    for data, targets in val_dl:\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = F.cross_entropy(output, targets)\n",
    "        val_loss += loss\n",
    "    val_ppl = val_loss.exp()\n",
    "    return val_loss.item(), val_ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:04.016366Z",
     "start_time": "2020-06-23T17:50:04.012467Z"
    }
   },
   "outputs": [],
   "source": [
    "def _descend(loss, optimizer, max_norm=None):\n",
    "    \"\"\"Perform one step of gradient descent.\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if max_norm:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:52:36.137114Z",
     "start_time": "2020-06-23T17:52:36.123725Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: why does the tutorial call init_hidden() inside each epoch?\n",
    "def train(train_dl, val_dl, model, optimizer, scheduler, n_epochs, max_norm, save_path):\n",
    "    \"\"\"Train a language model with the given optimizer and lr scheduler.\"\"\"\n",
    "    start = time.time()\n",
    "    losses, timestamps = [], []\n",
    "    best_val_loss, best_val_ppl = None, None\n",
    "    hidden = model.init_hidden(train_dl.dataset.n_sequences)\n",
    "        \n",
    "    for epoch in range(n_epochs): \n",
    "        model.train()\n",
    "        total_loss, N = 0, 0\n",
    "        \n",
    "        for data, targets in train_dl:\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(data, hidden)\n",
    "            print(f\"{output.shape= } {hidden.shape= }\")\n",
    "            loss = F.cross_entropy(output, targets)\n",
    "            _descend(loss, optimizer, max_norm)\n",
    "            total_loss += loss.item() * len(data)\n",
    "            N += len(data)\n",
    "        scheduler.step()\n",
    "\n",
    "        elapsed = (time.time() - start) / 60\n",
    "        train_loss = total_loss / N\n",
    "        losses.append(train_loss)\n",
    "        timestamps.append(elapsed)\n",
    "\n",
    "        val_loss, val_ppl = get_val_metrics(model, val_dl)\n",
    "        print(f\"{epoch= :3d} | {elapsed= :3.2f}min | {train_loss= :5.2f} | \"\n",
    "              f\"{val_loss= :5.2f} | {val_ppl= :8.2f}\")\n",
    "        \n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_loss_ppl = val_ppl\n",
    "            model.save(save_path)\n",
    "    \n",
    "    os.rename(save_path, f\"{save_path}_{best_val_loss:.2f}_loss_{best_val_loss_ppl:.2f}_ppl.pt\")\n",
    "    return losses, timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "TODO:\n",
    "\n",
    "* ASGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:04.039684Z",
     "start_time": "2020-06-23T17:50:04.033907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:05.114794Z",
     "start_time": "2020-06-23T17:50:04.042775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "dirpath = Path(\"../data/wikitext-2/\")\n",
    "corpus = WikiCorpus(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:05.118680Z",
     "start_time": "2020-06-23T17:50:05.116189Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Return data as a 2D-tensor and targets as a 1D tensor.\"\"\"\n",
    "    data = torch.stack([tup[0] for tup in batch])\n",
    "    targets = torch.cat([tup[1] for tup in batch])\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:05.130727Z",
     "start_time": "2020-06-23T17:50:05.121223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prep Datasets\n",
    "n_sequences = 50\n",
    "train_ds = LMDataset(corpus.train[:35*50], n_sequences, device)\n",
    "val_ds = LMDataset(corpus.valid[:35*50], n_sequences, device)\n",
    "test_ds = LMDataset(corpus.test, n_sequences, device)\n",
    "\n",
    "# Prep DataLoaders\n",
    "batch_size = 35\n",
    "train_dl = DataLoader(train_ds, batch_size, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(test_ds, batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:52:39.940956Z",
     "start_time": "2020-06-23T17:52:39.858431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set model params\n",
    "vocab_size = len(corpus.dictionary)\n",
    "emb_dim = 96\n",
    "hidden_dim = 96\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialise model\n",
    "model = GRULModel(vocab_size, emb_dim, hidden_dim, dropout=dropout, tie_weights=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:52:40.056135Z",
     "start_time": "2020-06-23T17:52:40.046979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to save model to\n",
    "save_path = Path(\"../models/tmp\")\n",
    "\n",
    "# Set training & schedluer params\n",
    "max_lr = 1\n",
    "max_norm = 0.25\n",
    "n_epochs = 40\n",
    "steps_per_epoch = len(train_dl)\n",
    "\n",
    "# Initialize optimizer & scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=1, momentum=0.9, nesterov=True)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=n_epochs, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:52:43.848569Z",
     "start_time": "2020-06-23T17:52:40.940292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape= torch.Size([1750, 33278]) hidden.shape= torch.Size([1, 50, 96])\n",
      "epoch=   0 | elapsed= 0.01min | train_loss= 10.42 | val_loss= 10.41 | val_ppl= 33267.00\n",
      "output.shape= torch.Size([1750, 33278]) hidden.shape= torch.Size([1, 50, 96])\n",
      "epoch=   1 | elapsed= 0.03min | train_loss= 10.42 | val_loss= 10.41 | val_ppl= 33153.23\n",
      "output.shape= torch.Size([1750, 33278]) hidden.shape= torch.Size([1, 50, 96])\n",
      "epoch=   2 | elapsed= 0.04min | train_loss= 10.41 | val_loss= 10.40 | val_ppl= 32865.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-071fafdd729c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-21ffef9e39e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dl, val_dl, model, optimizer, scheduler, n_epochs, max_norm, save_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepackage_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{output.shape= } {hidden.shape= }\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0m_descend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "losses, timestamps = train(train_dl, val_dl, model, optimizer, scheduler, n_epochs, max_norm, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:51:42.427316Z",
     "start_time": "2020-06-23T17:51:42.408066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 50]), torch.Size([1750]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, targets = next(iter(train_dl))\n",
    "data.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:40.621642Z",
     "start_time": "2020-06-23T17:50:40.609405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6655]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoffs = [int(0.2 * model.vocab_size)]\n",
    "cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T17:50:40.635649Z",
     "start_time": "2020-06-23T17:50:40.623977Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.AdaptiveLogSoftmaxWithLoss(model.hid_dim, model.vocab_size, cutoffs=cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
