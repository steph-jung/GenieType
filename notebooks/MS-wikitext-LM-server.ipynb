{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\">Objective</a></span></li><li><span><a href=\"#Data-Models\" data-toc-modified-id=\"Data-Models-2\">Data Models</a></span></li><li><span><a href=\"#Datasets\" data-toc-modified-id=\"Datasets-3\">Datasets</a></span></li><li><span><a href=\"#GRU\" data-toc-modified-id=\"GRU-4\">GRU</a></span></li><li><span><a href=\"#Training-Functions\" data-toc-modified-id=\"Training-Functions-5\">Training Functions</a></span></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-6\">Train Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-DataLoaders\" data-toc-modified-id=\"Prepare-DataLoaders-6.1\">Prepare DataLoaders</a></span></li><li><span><a href=\"#Training-Loop\" data-toc-modified-id=\"Training-Loop-6.2\">Training Loop</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:04.997598Z",
     "start_time": "2020-06-23T23:42:04.682655Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:05.019601Z",
     "start_time": "2020-06-23T23:42:04.999881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f73afd6cad0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The goal of this notebook is to train a language model from scratch on `wikitext-2`, which you can find [here](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). Our focus will be on getting the pre-processing and training loops working in the traditional, non-federated setting. In a separate notebook we'll do the same thing for the federated setting, which you can read more about in [this](https://arxiv.org/pdf/1811.03604.pdf) paper (which we'll refer to as the `Google Smart Keyboard Paper`).\n",
    "\n",
    "This notebook borrows heavily from [this](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html) pytorch tutorial, which is absolutely outstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Models\n",
    "\n",
    "Let's create data models for a corpus of text. The `Google Smart Keyboard Paper` uses a vocabulary of size `10,000` and inlcudes tokens for the beggining of sentence, end of setence, and out-of-vocab words. During inference time the probabilities for these tokens are ignored.\n",
    "\n",
    "TODO: \n",
    "\n",
    "* Limit vocab size\n",
    "* Add `<BOS>` tokens?\n",
    "* Sort words by descending frequency to enable training with `AdaptiveLogSoftMax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:05.030124Z",
     "start_time": "2020-06-23T23:42:05.022623Z"
    }
   },
   "outputs": [],
   "source": [
    "class DescendingDict:\n",
    "    \"\"\"Model a vocab as a mapping word <-> index, ordered by descending frequency.\"\"\"\n",
    "    \n",
    "    def __init__(self, words, vocab_size=None):\n",
    "        word2freq = Counter(words).most_common(vocab_size)\n",
    "        self.idx2word = [tup[0] for tup in word2freq]\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.idx2word)}\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "                                \n",
    "    def get_index(self, word):\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def get_indices(self, words):\n",
    "        return [self.word2idx[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:05.043687Z",
     "start_time": "2020-06-23T23:42:05.033874Z"
    }
   },
   "outputs": [],
   "source": [
    "class WikiCorpus:\n",
    "    \"\"\"Encode a corpus of text already processed in the wikitext style.\"\"\"\n",
    "    \n",
    "    def __init__(self, dirpath, vocab_size=None):\n",
    "        \"\"\"Build a corpus given a dir with train, valid, and test .txt files.\"\"\"\n",
    "        words = self.tokenize(os.path.join(dirpath, \"train.txt\"))\n",
    "        self.dictionary = DescendingDict(words, vocab_size)\n",
    "        self.train = self.vectorize(os.path.join(dirpath, \"train.txt\"))\n",
    "        self.valid = self.vectorize(os.path.join(dirpath, \"valid.txt\"))\n",
    "        self.test = self.vectorize(os.path.join(dirpath, \"test.txt\"))\n",
    "        \n",
    "    def tokenize(self, fpath):\n",
    "        \"\"\"Split on new lines and append <eos> tokens.\"\"\"\n",
    "        words = []\n",
    "        with open(fpath, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words += line.split() + [\"<eos>\"]\n",
    "        return words\n",
    "        \n",
    "    def vectorize(self, fpath):\n",
    "        \"\"\"Return a tensor of indexes encoding the words in a file.\"\"\"\n",
    "        idxs = []\n",
    "        with open(fpath, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + [\"<eos>\"]\n",
    "                idxs.extend(self.dictionary.get_indices(words))\n",
    "        return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:05.053900Z",
     "start_time": "2020-06-23T23:42:05.047839Z"
    }
   },
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    \"\"\"Dataset for language models.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, n_partitions, device):\n",
    "        \"\"\"Partition text data into sequences and move to device.\"\"\"\n",
    "        self.data = self.partition(data, n_partitions, device)\n",
    "        self.n_partitions = n_partitions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Return the i-th element from all sequences along with their targets.\"\"\"\n",
    "        if i == len(self.data) - 1:\n",
    "            return self.data[i-1], self.data[i]\n",
    "        return self.data[i], self.data[i + 1]\n",
    "        \n",
    "    def partition(self, data, n_partitions, device):\n",
    "        \"\"\"Re-shape data to have ``n_partitions`` columns (discards remainder).\"\"\"\n",
    "        n_rows = len(data) // n_partitions\n",
    "        data = data[:n_rows * n_partitions]\n",
    "        data = data.view(n_partitions, -1).t().contiguous()\n",
    "        return data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# GRU \n",
    "\n",
    "We'll use a single layer `GRU` with somewhere around `600` hidden units. We'll tie the embedding weights to the softmax layer, as described in [this](https://arxiv.org/pdf/1611.01462.pdf) paper.\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Add adpative softmax\n",
    "* Add Layer Normalization\n",
    "* Variational dropout\n",
    "* Weight dropping\n",
    "* Ignore `<unk>`, `<eos>` at inference time\n",
    "* Use neural cache\n",
    "* Simplify `reapackage_hidden` procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:05.070967Z",
     "start_time": "2020-06-23T23:42:05.056204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class GRULModel(nn.Module):\n",
    "    \"\"\"Language model with an encoder, GRU module, and a decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_sz, emb_dim, hid_dim, n_layers=1, dropout=0.5, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, n_layers)\n",
    "        self.decoder = nn.Linear(hid_dim, vocab_size)\n",
    "                \n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.init_weights()\n",
    "        if tie_weights:\n",
    "            assert hid_dim == emb_dim, f\"{hid_dim= } must match {emb_dim= }!\"\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "    \n",
    "    #TODO: why do we initialise with zero bias?\n",
    "    def init_weights(self, k=0.1):\n",
    "        \"\"\"Initialise weights from a uniform distribution U(-k, k).\"\"\"\n",
    "        self.encoder.weight.data.uniform_(-k, k)\n",
    "        self.decoder.weight.data.uniform_(-k, k)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        \n",
    "    def init_hidden(self, n_partitions):\n",
    "        \"\"\"Initialise hidden weights.\"\"\"\n",
    "        weights = next(self.parameters())\n",
    "        return weights.new_zeros(self.n_layers, n_partitions, self.hid_dim)\n",
    "    \n",
    "    def repackage_hidden(self, hidden):\n",
    "        \"\"\"Return hidden states in new tensors detatched from their history.\"\"\"\n",
    "        if isinstance(hidden, torch.Tensor):\n",
    "            return hidden.detach()\n",
    "        return tuple(repackage_hidden(v) for v in hidden)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.gru(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.vocab_sz)\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def predict(self, x, ignore=None):\n",
    "        \"\"\"Return the most likely next word, ignoring certain tokens.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Serialise weights to disc.\"\"\"\n",
    "        torch.save(self.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainers\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Add `critetion` param to `Trainer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:05.080027Z",
     "start_time": "2020-06-23T23:42:05.074157Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Abstract base class for training a general neural model.\"\"\"\n",
    "    \n",
    "    def __init__(self, train_dl, val_dl, model, optimizer, scheduler):\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def _descend(self, loss, max_norm=None):\n",
    "        \"\"\"Perform one step of gradient descent.\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if max_norm:\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:27.927139Z",
     "start_time": "2020-06-23T23:42:27.904933Z"
    }
   },
   "outputs": [],
   "source": [
    "class LMTrainer(Trainer):\n",
    "    \"\"\"Container for training a language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, train_dl, val_dl, model, optimizer, scheduler):\n",
    "        super().__init__(train_dl, val_dl, model, optimizer, scheduler)\n",
    "        self.losses, self.timestamps = [], []\n",
    "                \n",
    "    def _train(self, hidden, max_norm=None):\n",
    "        \"\"\"Train for a single epoch and return the loss.\"\"\"\n",
    "        total_loss, N = 0, 0\n",
    "        for data, targets in self.train_dl:\n",
    "            hidden = self.model.repackage_hidden(hidden)\n",
    "            output, hidden = self.model(data, hidden)\n",
    "            loss = F.cross_entropy(output, targets)\n",
    "            self._descend(loss, max_norm)\n",
    "            total_loss += loss.item() * len(data)\n",
    "            N += len(data)\n",
    "        return total_loss / N\n",
    "        \n",
    "    #TODO: why does the tutorial call init_hidden() inside each epoch?\n",
    "    def train(self, n_epochs, max_norm=None, save_path=\"/tmp/lmodel\"):\n",
    "        \"\"\"Train a language model with optional graident clipping.\"\"\"\n",
    "        start = time.time()\n",
    "        best_val_loss, best_val_ppl = None, None\n",
    "        hidden = self.model.init_hidden(self.train_dl.dataset.n_partitions)\n",
    "\n",
    "        for epoch in range(n_epochs): \n",
    "            self.model.train()\n",
    "            train_loss = self._train(hidden, max_norm)  \n",
    "            scheduler.step()\n",
    "\n",
    "            elapsed = (time.time() - start) / 60\n",
    "            self.timestamps.append(elapsed)\n",
    "            self.losses.append(train_loss)\n",
    "\n",
    "            val_loss, val_ppl = self.get_val_metrics()\n",
    "            print(f\"{epoch= :3d} | {elapsed= :4.2f}min | {train_loss= :5.2f} | \"\n",
    "                  f\"{val_loss= :5.2f} | {val_ppl= :8.2f}\")\n",
    "\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_val_loss_ppl = val_ppl\n",
    "                model.save(save_path)\n",
    "        os.rename(save_path, f\"{save_path}_{best_val_loss:.2f}_loss_{best_val_loss_ppl:.2f}_ppl.pt\")\n",
    "        \n",
    "    def get_val_metrics(self):\n",
    "        \"\"\"Return validation loss and perplexity.\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        hidden = self.model.init_hidden(self.val_dl.dataset.n_partitions)\n",
    "\n",
    "        for data, targets in self.val_dl:\n",
    "            output, hidden = self.model(data, hidden)\n",
    "            loss = F.cross_entropy(output, targets)\n",
    "            val_loss += loss\n",
    "        val_ppl = val_loss.exp()\n",
    "        return val_loss.item(), val_ppl.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "TODO:\n",
    "\n",
    "* ASGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:05.104790Z",
     "start_time": "2020-06-23T23:42:05.100507Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:06.402823Z",
     "start_time": "2020-06-23T23:42:05.108993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "dirpath = Path(\"../data/wikitext-2/\")\n",
    "corpus = WikiCorpus(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:06.413961Z",
     "start_time": "2020-06-23T23:42:06.405778Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Return data as a 2D-tensor and targets as a 1D tensor.\"\"\"\n",
    "    data = torch.stack([tup[0] for tup in batch])\n",
    "    targets = torch.cat([tup[1] for tup in batch])\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:06.428320Z",
     "start_time": "2020-06-23T23:42:06.420720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prep Datasets\n",
    "n_sequences = 50\n",
    "train_ds = LMDataset(corpus.train[:35*50], n_sequences, device)\n",
    "val_ds = LMDataset(corpus.valid[:35*50], n_sequences, device)\n",
    "test_ds = LMDataset(corpus.test, n_sequences, device)\n",
    "\n",
    "# Prep DataLoaders\n",
    "batch_size = 35\n",
    "train_dl = DataLoader(train_ds, batch_size, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(test_ds, batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:43:21.002623Z",
     "start_time": "2020-06-23T23:43:20.920544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set model params\n",
    "vocab_size = len(corpus.dictionary)\n",
    "emb_dim = 96\n",
    "hidden_dim = 96\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialise model\n",
    "model = GRULModel(vocab_size, emb_dim, hidden_dim, dropout=dropout, tie_weights=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:43:21.137777Z",
     "start_time": "2020-06-23T23:43:21.125969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to save model to\n",
    "save_path = Path(\"../models/tmp\")\n",
    "\n",
    "# Set training & schedluer params\n",
    "max_lr = 1\n",
    "max_norm = 0.25\n",
    "n_epochs = 40\n",
    "steps_per_epoch = len(train_dl)\n",
    "\n",
    "# Initialize optimizer & scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=1, momentum=0.9, nesterov=True)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=n_epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = LMTrainer(train_dl, val_dl, model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:43:57.371083Z",
     "start_time": "2020-06-23T23:43:21.321121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=   0 | elapsed= 0.01min | train_loss= 10.42 | val_loss= 10.42 | val_ppl= 33566.98\n",
      "epoch=   1 | elapsed= 0.03min | train_loss= 10.42 | val_loss= 10.42 | val_ppl= 33454.07\n",
      "epoch=   2 | elapsed= 0.04min | train_loss= 10.42 | val_loss= 10.41 | val_ppl= 33174.87\n",
      "epoch=   3 | elapsed= 0.06min | train_loss= 10.41 | val_loss= 10.39 | val_ppl= 32596.22\n",
      "epoch=   4 | elapsed= 0.07min | train_loss= 10.39 | val_loss= 10.36 | val_ppl= 31617.87\n",
      "epoch=   5 | elapsed= 0.09min | train_loss= 10.35 | val_loss= 10.31 | val_ppl= 30154.77\n",
      "epoch=   6 | elapsed= 0.10min | train_loss= 10.30 | val_loss= 10.24 | val_ppl= 28087.32\n",
      "epoch=   7 | elapsed= 0.12min | train_loss= 10.23 | val_loss= 10.13 | val_ppl= 25083.55\n",
      "epoch=   8 | elapsed= 0.13min | train_loss= 10.10 | val_loss=  9.91 | val_ppl= 20231.46\n",
      "epoch=   9 | elapsed= 0.15min | train_loss=  9.87 | val_loss=  9.53 | val_ppl= 13780.87\n",
      "epoch=  10 | elapsed= 0.16min | train_loss=  9.47 | val_loss=  9.09 | val_ppl=  8875.25\n",
      "epoch=  11 | elapsed= 0.18min | train_loss=  9.03 | val_loss=  8.96 | val_ppl=  7814.49\n",
      "epoch=  12 | elapsed= 0.19min | train_loss=  8.94 | val_loss=  8.66 | val_ppl=  5784.46\n",
      "epoch=  13 | elapsed= 0.21min | train_loss=  8.57 | val_loss=  8.51 | val_ppl=  4958.22\n",
      "epoch=  14 | elapsed= 0.23min | train_loss=  8.38 | val_loss=  8.37 | val_ppl=  4297.63\n",
      "epoch=  15 | elapsed= 0.24min | train_loss=  8.20 | val_loss=  8.25 | val_ppl=  3832.43\n",
      "epoch=  16 | elapsed= 0.26min | train_loss=  8.04 | val_loss=  8.16 | val_ppl=  3493.23\n",
      "epoch=  17 | elapsed= 0.27min | train_loss=  7.87 | val_loss=  8.10 | val_ppl=  3302.48\n",
      "epoch=  18 | elapsed= 0.29min | train_loss=  7.70 | val_loss=  8.06 | val_ppl=  3158.60\n",
      "epoch=  19 | elapsed= 0.30min | train_loss=  7.57 | val_loss=  8.02 | val_ppl=  3055.72\n",
      "epoch=  20 | elapsed= 0.32min | train_loss=  7.46 | val_loss=  8.00 | val_ppl=  2973.79\n",
      "epoch=  21 | elapsed= 0.34min | train_loss=  7.32 | val_loss=  7.97 | val_ppl=  2903.42\n",
      "epoch=  22 | elapsed= 0.35min | train_loss=  7.21 | val_loss=  7.96 | val_ppl=  2866.38\n",
      "epoch=  23 | elapsed= 0.37min | train_loss=  7.13 | val_loss=  7.95 | val_ppl=  2822.90\n",
      "epoch=  24 | elapsed= 0.38min | train_loss=  7.04 | val_loss=  7.94 | val_ppl=  2816.55\n",
      "epoch=  25 | elapsed= 0.40min | train_loss=  6.95 | val_loss=  7.94 | val_ppl=  2801.32\n",
      "epoch=  26 | elapsed= 0.41min | train_loss=  6.89 | val_loss=  7.94 | val_ppl=  2813.15\n",
      "epoch=  27 | elapsed= 0.43min | train_loss=  6.81 | val_loss=  7.94 | val_ppl=  2803.12\n",
      "epoch=  28 | elapsed= 0.44min | train_loss=  6.77 | val_loss=  7.94 | val_ppl=  2807.45\n",
      "epoch=  29 | elapsed= 0.46min | train_loss=  6.71 | val_loss=  7.94 | val_ppl=  2802.38\n",
      "epoch=  30 | elapsed= 0.47min | train_loss=  6.66 | val_loss=  7.94 | val_ppl=  2803.07\n",
      "epoch=  31 | elapsed= 0.48min | train_loss=  6.65 | val_loss=  7.94 | val_ppl=  2803.64\n",
      "epoch=  32 | elapsed= 0.50min | train_loss=  6.61 | val_loss=  7.94 | val_ppl=  2804.40\n",
      "epoch=  33 | elapsed= 0.51min | train_loss=  6.58 | val_loss=  7.94 | val_ppl=  2804.71\n",
      "epoch=  34 | elapsed= 0.52min | train_loss=  6.58 | val_loss=  7.94 | val_ppl=  2805.33\n",
      "epoch=  35 | elapsed= 0.54min | train_loss=  6.56 | val_loss=  7.94 | val_ppl=  2805.26\n",
      "epoch=  36 | elapsed= 0.55min | train_loss=  6.56 | val_loss=  7.94 | val_ppl=  2805.42\n",
      "epoch=  37 | elapsed= 0.57min | train_loss=  6.54 | val_loss=  7.94 | val_ppl=  2805.15\n",
      "epoch=  38 | elapsed= 0.58min | train_loss=  6.52 | val_loss=  7.94 | val_ppl=  2805.04\n",
      "epoch=  39 | elapsed= 0.60min | train_loss=  6.53 | val_loss=  7.94 | val_ppl=  2805.04\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train(n_epochs, max_norm, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:07.214254Z",
     "start_time": "2020-06-23T23:42:04.680Z"
    }
   },
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_dl))\n",
    "data.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:07.216645Z",
     "start_time": "2020-06-23T23:42:04.681Z"
    }
   },
   "outputs": [],
   "source": [
    "cutoffs = [int(0.2 * model.vocab_size)]\n",
    "cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T23:42:07.218307Z",
     "start_time": "2020-06-23T23:42:04.683Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.AdaptiveLogSoftmaxWithLoss(model.hid_dim, model.vocab_size, cutoffs=cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
